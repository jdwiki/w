<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>apache-kıvılcım Eğitimi on </title>
    <link>https://www.wikiod.com/tr/docs/apache-spark/</link>
    <description>Recent content in apache-kıvılcım Eğitimi on </description>
    <generator>Hugo -- gohugo.io</generator><atom:link href="https://www.wikiod.com/tr/docs/apache-spark/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>apache-spark&#39;ı kullanmaya başlama</title>
      <link>https://www.wikiod.com/tr/apache-spark/apache-spark-kullanmaya-baslama/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://www.wikiod.com/tr/apache-spark/apache-spark-kullanmaya-baslama/</guid>
      <description>Giriiş # Prototip:
toplama(zeroValue, seqOp, combOp)
Tanım:
&amp;lsquo;aggregate()&amp;rsquo;, bir RDD almanızı ve orijinal RDD&amp;rsquo;de depolanandan farklı türde tek bir değer oluşturmanızı sağlar.
Parametreler:
zeroValue: İstediğiniz şekilde, sonucunuz için başlatma değeri format. seqOp: RDD kayıtlarına uygulamak istediğiniz işlem. için bir kez çalışır every record in a partition. combOp: Sonuçlanan nesnelerin (her bölüm için bir tane), gets combined. Örnek:
Bir listenin toplamını ve o listenin uzunluğunu hesaplayın. Sonucu bir çift &amp;ldquo;(toplam, uzunluk)&amp;rdquo; olarak döndürün.</description>
    </item>
    
    <item>
      <title>Spark SQL&#39;de Pencere İşlevleri</title>
      <link>https://www.wikiod.com/tr/apache-spark/spark-sqlde-pencere-islevleri/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://www.wikiod.com/tr/apache-spark/spark-sqlde-pencere-islevleri/</guid>
      <description>Giriiş # Pencere işlevleri, topluca pencere olarak adlandırılan bir dizi satır üzerinde işlemler (genellikle toplama) yapmak için kullanılır. Pencere işlevleri, Spark 1.4 veya sonraki sürümlerde çalışır. Pencere işlevleri, yerleşik işlevlerden veya substr veya round gibi UDF&amp;rsquo;lerden daha fazla işlem sağlar (Spark 1.4&amp;rsquo;ten önce yaygın olarak kullanılır). Pencere işlevleri, Spark SQL kullanıcılarının belirli bir satırın sıralaması veya bir dizi girdi satırı üzerinde hareketli bir ortalama gibi sonuçları hesaplamasına olanak tanır. Spark&amp;rsquo;ın SQL ve DataFrame API&amp;rsquo;lerinin ifadesini önemli ölçüde artırırlar.</description>
    </item>
    
    <item>
      <title>bölümler</title>
      <link>https://www.wikiod.com/tr/apache-spark/bolumler/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://www.wikiod.com/tr/apache-spark/bolumler/</guid>
      <description>Bölümlerin sayısı, bir uygulamanın performansı ve/veya başarılı bir şekilde sonlandırılması için kritik öneme sahiptir.
Esnek Dağıtılmış Veri Kümesi (RDD), Spark&amp;rsquo;ın ana soyutlamasıdır. Bir RDD bölümlere ayrılır; bu, bir bölümün veri kümesinin bir parçası, bir dilimi veya başka bir deyişle, bir parçası olduğu anlamına gelir.
Bölüm sayısı ne kadar fazlaysa, her bölümün boyutu o kadar küçüktür.
Ancak, çok sayıda bölümün, önemli miktarda meta veri tutması gereken Hadoop Dağıtılmış Dosya Sistemi (HDFS) üzerinde çok fazla baskı oluşturduğuna dikkat edin.</description>
    </item>
    
    <item>
      <title>Paylaşılan Değişkenler</title>
      <link>https://www.wikiod.com/tr/apache-spark/paylaslan-degiskenler/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://www.wikiod.com/tr/apache-spark/paylaslan-degiskenler/</guid>
      <description>Akümülatörler # Akümülatörler, &amp;ldquo;SparkContext.accumulator&amp;rdquo; ile oluşturulabilen salt okunur değişkenlerdir:
val accumulator = sc.accumulator(0, name = &amp;quot;My accumulator&amp;quot;) // name is optional += ile değiştirildi:
val someRDD = sc.parallelize(Array(1, 2, 3, 4)) someRDD.foreach(element =&amp;gt; accumulator += element) ve &amp;ldquo;değer&amp;rdquo; yöntemiyle erişildi:
accumulator.value // &#39;value&#39; is now equal to 10 Akümülatörleri kullanmak, Spark&amp;rsquo;ın dönüşümler için en az bir kez çalışma garantisi nedeniyle karmaşıktır. Herhangi bir nedenle bir dönüşümün yeniden hesaplanması gerekiyorsa, bu dönüşüm sırasında akümülatör güncellemeleri tekrarlanacaktır.</description>
    </item>
    
    <item>
      <title>Spark Streaming&#39;de durum bilgisi olan işlemler</title>
      <link>https://www.wikiod.com/tr/apache-spark/spark-streamingde-durum-bilgisi-olan-islemler/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://www.wikiod.com/tr/apache-spark/spark-streamingde-durum-bilgisi-olan-islemler/</guid>
      <description>PairDStreamFunctions.updateStateByKey # Anahtara göre &amp;ldquo;updateState&amp;rdquo;, yaklaşan verilere dayalı durum bilgisi olan bir &amp;ldquo;DStream&amp;rdquo; oluşturmak için kullanılabilir. Bir işlev gerektirir:
object UpdateStateFunctions { def updateState(current: Seq[Double], previous: Option[StatCounter]) = { previous.map(s =&amp;gt; s.merge(current)).orElse(Some(StatCounter(current))) } } bu, &amp;ldquo;geçerli&amp;rdquo; değerlerin bir dizisini, önceki durumun bir &amp;ldquo;Seçenek&amp;quot;ini alır ve güncellenmiş durumun bir &amp;ldquo;Seçenek&amp;quot;ini döndürür. Hepsini bir araya getirmek:
import org.apache.spark._ import org.apache.spark.streaming.dstream.DStream import scala.collection.mutable.Queue import org.apache.spark.util.StatCounter import org.apache.spark.streaming._ object UpdateStateByKeyApp { def main(args: Array[String]) { val sc = new SparkContext(&amp;quot;local&amp;quot;, &amp;quot;updateStateByKey&amp;quot;, new SparkConf()) val ssc = new StreamingContext(sc, Seconds(10)) ssc.</description>
    </item>
    
    <item>
      <title>Spark 1.6&#39;dan Spark 2.0&#39;a Geçiş</title>
      <link>https://www.wikiod.com/tr/apache-spark/spark-16dan-spark-20a-gecis/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://www.wikiod.com/tr/apache-spark/spark-16dan-spark-20a-gecis/</guid>
      <description>Spark 2.0 piyasaya sürüldü ve birçok geliştirme ve yeni özellik içeriyor. Spark 1.6 kullanıyorsanız ve şimdi uygulamanızı Spark 2.0 kullanacak şekilde yükseltmek istiyorsanız, API&amp;rsquo;deki bazı değişiklikleri hesaba katmanız gerekir. Aşağıda kodda yapılması gereken bazı değişiklikler bulunmaktadır.
build.sbt dosyasını güncelleyin # build.sbt dosyasını şu şekilde güncelleyin:
scalaVersion := &amp;quot;2.11.8&amp;quot; // Make sure to have installed Scala 11 sparkVersion := &amp;quot;2.0.0&amp;quot; // Make sure to have installed Spark 2.0 &amp;lsquo;sbt paketi&amp;rsquo; ile derlerken, &amp;lsquo;.</description>
    </item>
    
    <item>
      <title>Birim testleri</title>
      <link>https://www.wikiod.com/tr/apache-spark/birim-testleri/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://www.wikiod.com/tr/apache-spark/birim-testleri/</guid>
      <description>Kelime sayısı birim testi (Scala + JUnit) # Örneğin, &amp;lsquo;countWords&amp;rsquo; yöntemiyle &amp;lsquo;WordCountService&amp;rsquo;imiz var:
class WordCountService { def countWords(url: String): Map[String, Int] = { val sparkConf = new SparkConf().setMaster(&amp;quot;spark://somehost:7077&amp;quot;).setAppName(&amp;quot;WordCount&amp;quot;)) val sc = new SparkContext(sparkConf) val textFile = sc.textFile(url) textFile.flatMap(line =&amp;gt; line.split(&amp;quot; &amp;quot;)) .map(word =&amp;gt; (word, 1)) .reduceByKey(_ + _).collect().toMap } } Bu hizmet çok çirkin görünüyor ve birim testi için uyarlanmamış. SparkContext bu hizmete enjekte edilmelidir. En sevdiğiniz DI çerçevesiyle erişilebilir, ancak basitlik için yapıcı kullanılarak uygulanacaktır:</description>
    </item>
    
    <item>
      <title>Kıvılcım Başlatıcı</title>
      <link>https://www.wikiod.com/tr/apache-spark/kvlcm-baslatc/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://www.wikiod.com/tr/apache-spark/kvlcm-baslatc/</guid>
      <description>Spark Launcher, geliştiricinin gönderilen kıvılcım işinin durumunu yoklamasına yardımcı olabilir. Temel olarak oylanabilecek sekiz durum vardır. Aşağıda anlamları ile birlikte listelenmiştir:
/** The application has not reported back yet. */ UNKNOWN(false), /** The application has connected to the handle. */ CONNECTED(false), /** The application has been submitted to the cluster. */ SUBMITTED(false), /** The application is running. */ RUNNING(false), /** The application finished with a successful status. */ FINISHED(true), /** The application finished with a failed status.</description>
    </item>
    
    <item>
      <title>JSON&#39;u Spark&#39;ta Kullanma</title>
      <link>https://www.wikiod.com/tr/apache-spark/jsonu-sparkta-kullanma/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://www.wikiod.com/tr/apache-spark/jsonu-sparkta-kullanma/</guid>
      <description>JSON&amp;rsquo;u Gson ile Özel Bir Sınıfla Eşleme # Gson ile, JSON veri kümesini okuyabilir ve bunları özel bir MyClass sınıfına eşleyebilirsiniz.
Gson serileştirilebilir olmadığından, her yürütücünün kendi Gson nesnesine ihtiyacı vardır. Ayrıca, yürütücüler arasında iletilebilmesi için Sınıfım serileştirilebilir olmalıdır.
json dosyası olarak sunulan dosya(lar)ın tipik bir JSON dosyası olmadığını unutmayın. Her satır, ayrı, bağımsız bir geçerli JSON nesnesi içermelidir. Sonuç olarak, normal bir çok satırlı JSON dosyası çoğunlukla başarısız olur.</description>
    </item>
    
    <item>
      <title>Scala&#39;daki metin dosyaları ve işlemler</title>
      <link>https://www.wikiod.com/tr/apache-spark/scaladaki-metin-dosyalar-ve-islemler/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://www.wikiod.com/tr/apache-spark/scaladaki-metin-dosyalar-ve-islemler/</guid>
      <description>Metin dosyalarını okuma ve üzerlerinde işlem yapma.
Örnek kullanım # Metin dosyasını yoldan okuyun:
val sc: org.apache.spark.SparkContext = ??? sc.textFile(path=&amp;quot;/path/to/input/file&amp;quot;) Dosyaları joker karakterler kullanarak okuyun:
sc.textFile(path=&amp;quot;/path/to/*/*&amp;quot;) Minimum bölüm sayısını belirten dosyaları okuyun:
sc.textFile(path=&amp;quot;/path/to/input/file&amp;quot;, minPartitions=3) textFile() ile okunan iki dosyayı birleştir # Spark&amp;rsquo;a katılır:
Metin dosyasını oku 1
val txt1=sc.textFile(path=&amp;quot;/path/to/input/file1&amp;quot;) Örneğin:
A B 1 2 3 4 Metin dosyasını oku 2
val txt2=sc.textFile(path=&amp;quot;/path/to/input/file2&amp;quot;) Örneğin:
A C 1 5 3 6 Katılın ve sonucu yazdırın.</description>
    </item>
    
  </channel>
</rss>
