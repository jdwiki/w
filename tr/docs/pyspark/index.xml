<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>pyspark Eğitimi on </title>
    <link>https://www.wikiod.com/tr/docs/pyspark/</link>
    <description>Recent content in pyspark Eğitimi on </description>
    <generator>Hugo -- gohugo.io</generator><atom:link href="https://www.wikiod.com/tr/docs/pyspark/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>pyspark&#39;ı kullanmaya başlama</title>
      <link>https://www.wikiod.com/tr/pyspark/pyspark-kullanmaya-baslama/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://www.wikiod.com/tr/pyspark/pyspark-kullanmaya-baslama/</guid>
      <description>Pyspark&amp;rsquo;ta Örnek Kelime Sayısı # Temeldeki örnek, resmi pyspark belgelerinde verilen örnektir. Bu örneğe ulaşmak için lütfen buraya tıklayın.
# the first step involves reading the source text file from HDFS text_file = sc.textFile(&amp;quot;hdfs://...&amp;quot;) # this step involves the actual computation for reading the number of words in the file # flatmap, map and reduceByKey are all spark RDD functions counts = text_file.flatMap(lambda line: line.split(&amp;quot; &amp;quot;)) \ .map(lambda word: (word, 1)) \ .</description>
    </item>
    
  </channel>
</rss>
