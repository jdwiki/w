<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>didacticiel hadoop on </title>
    <link>https://www.wikiod.com/fr/docs/hadoop/</link>
    <description>Recent content in didacticiel hadoop on </description>
    <generator>Hugo -- gohugo.io</generator><atom:link href="https://www.wikiod.com/fr/docs/hadoop/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Premiers pas avec hadoop</title>
      <link>https://www.wikiod.com/fr/hadoop/premiers-pas-avec-hadoop/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://www.wikiod.com/fr/hadoop/premiers-pas-avec-hadoop/</guid>
      <description>Installation de Hadoop sur Ubuntu # Création d&amp;rsquo;un utilisateur Hadoop : # sudo addgroup hadoop Ajout d&amp;rsquo;un utilisateur : # sudo adduser --ingroup hadoop hduser001 [![entrez la description de l&amp;rsquo;image ici][1]][1]
Configuration de SSH : # su -hduser001 ssh-keygen -t rsa -P &amp;quot;&amp;quot; cat .ssh/id rsa.pub &amp;gt;&amp;gt; .ssh/authorized_keys Remarque : Si vous obtenez des erreurs [bash : .ssh/authorized_keys : aucun fichier ou répertoire de ce type] lors de l&amp;rsquo;écriture de la clé autorisée.</description>
    </item>
    
    <item>
      <title>Commandes Hadoop</title>
      <link>https://www.wikiod.com/fr/hadoop/commandes-hadoop/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://www.wikiod.com/fr/hadoop/commandes-hadoop/</guid>
      <description>Syntaxe # Commandes Hadoop v1 : hadoop fs -&amp;lt;commande&amp;gt;
Commandes Hadoop v2 : hdfs dfs -&amp;lt;commande&amp;gt;
Commandes Hadoop v1 # 1. Imprimez la version Hadoop # hadoop version 2. Lister le contenu du répertoire racine dans HDFS # # hadoop fs -ls / 3. Indiquez la quantité d&amp;rsquo;espace utilisé et # disponible sur le système de fichiers actuellement monté # # hadoop fs -df hdfs:/ 4. Comptez le nombre de répertoires, de fichiers et d&amp;rsquo;octets sous # les chemins qui correspondent au modèle de fichier spécifié # # hadoop fs -count hdfs:/ 5.</description>
    </item>
    
    <item>
      <title>Qu&#39;est-ce qu&#39;HDFS ?</title>
      <link>https://www.wikiod.com/fr/hadoop/quest-ce-quhdfs/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://www.wikiod.com/fr/hadoop/quest-ce-quhdfs/</guid>
      <description>Une bonne explication de HDFS et de son fonctionnement.
La syntaxe doit contenir les commandes qui peuvent être utilisées dans HDFS.
Recherche de fichiers dans HDFS # Pour rechercher un fichier dans le système de fichiers distribué Hadoop :
hdfs dfs -ls -R / | grep [search_term] Dans la commande ci-dessus,
-ls est pour lister les fichiers
-R est pour récursif (itérer dans les sous-répertoires)
/ signifie depuis le répertoire racine</description>
    </item>
    
    <item>
      <title>Présentation de MapReduce</title>
      <link>https://www.wikiod.com/fr/hadoop/presentation-de-mapreduce/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://www.wikiod.com/fr/hadoop/presentation-de-mapreduce/</guid>
      <description>Syntaxe # Pour exécuter l&amp;rsquo;exemple, la syntaxe de la commande est :
bin/hadoop jar hadoop-*-examples.jar wordcount [-m &amp;lt;#maps&amp;gt;] [-r &amp;lt;#reducers&amp;gt;] &amp;lt;in-dir&amp;gt; &amp;lt;out-dir&amp;gt; Pour copier des données dans HDFS (depuis le local) :
bin/hadoop dfs -mkdir &amp;lt;hdfs-dir&amp;gt; //not required in hadoop 0.17.2 and later bin/hadoop dfs -copyFromLocal &amp;lt;local-dir&amp;gt; &amp;lt;hdfs-dir&amp;gt; Programme de comptage de mots utilisant MapReduce dans Hadoop.
Programme de comptage de mots (en Java et Python) # Le programme de comptage de mots est comme le programme &amp;ldquo;Hello World&amp;rdquo; dans MapReduce.</description>
    </item>
    
    <item>
      <title>Données de chargement Hadoop</title>
      <link>https://www.wikiod.com/fr/hadoop/donnees-de-chargement-hadoop/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://www.wikiod.com/fr/hadoop/donnees-de-chargement-hadoop/</guid>
      <description>Charger des données dans hadoop hdfs # ÉTAPE 1 : CRÉER UN RÉPERTOIRE DANS HDFS, TÉLÉCHARGER UN FICHIER ET LISTER LE CONTENU
Apprenons en écrivant la syntaxe. Vous pourrez copier et coller les exemples de commandes suivants dans votre terminal :
hadoop fs -mkdir : # Prend les URI du chemin comme argument et crée un répertoire ou plusieurs répertoires.
Utilisation : # # hadoop fs -mkdir &amp;lt;paths&amp;gt; Exemple: # hadoop fs -mkdir /user/hadoop hadoop fs -mkdir /user/hadoop/dir1 /user/hadoop/dir2 /user/hadoop/dir3 hadoop fs -put : # Copie un seul fichier src ou plusieurs fichiers src du système de fichiers local vers le système de fichiers distribué Hadoop.</description>
    </item>
    
    <item>
      <title>teinte</title>
      <link>https://www.wikiod.com/fr/hadoop/teinte/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://www.wikiod.com/fr/hadoop/teinte/</guid>
      <description>Hue est une interface utilisateur permettant de se connecter et de travailler avec la plupart des technologies Bigdata couramment utilisées telles que HDFS, Hive, Spark, Hbase, Sqoop, Impala, Pig, Oozie, etc. Hue prend également en charge l&amp;rsquo;exécution de requêtes sur des bases de données relationnelles.
Hue, une application Web Django, a été principalement conçue comme un atelier pour exécuter des requêtes Hive. Plus tard, la fonctionnalité de Hue a augmenté pour prendre en charge différents composants de l&amp;rsquo;écosystème Hadoop.</description>
    </item>
    
    <item>
      <title>Débogage du code Java Hadoop MR dans l&#39;environnement de développement Eclipse local.</title>
      <link>https://www.wikiod.com/fr/hadoop/debogage-du-code-java-hadoop-mr-dans-lenvironnement-de-developpement-eclipse-local/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://www.wikiod.com/fr/hadoop/debogage-du-code-java-hadoop-mr-dans-lenvironnement-de-developpement-eclipse-local/</guid>
      <description>La chose fondamentale à retenir ici est que le débogage d&amp;rsquo;un travail Hadoop MR sera similaire à n&amp;rsquo;importe quelle application déboguée à distance dans Eclipse.
Un débogueur ou un outil de débogage est un programme informatique utilisé pour tester et déboguer d&amp;rsquo;autres programmes (le programme &amp;ldquo;cible&amp;rdquo;). Il est très utile spécialement pour un environnement Hadoop où il y a peu de place pour l&amp;rsquo;erreur et une petite erreur peut entraîner une perte énorme.</description>
    </item>
    
  </channel>
</rss>
