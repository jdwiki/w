<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Tutoriel sqoop on </title>
    <link>https://www.wikiod.com/fr/docs/sqoop/</link>
    <description>Recent content in Tutoriel sqoop on </description>
    <generator>Hugo -- gohugo.io</generator><atom:link href="https://www.wikiod.com/fr/docs/sqoop/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Premiers pas avec sqoop</title>
      <link>https://www.wikiod.com/fr/sqoop/premiers-pas-avec-sqoop/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://www.wikiod.com/fr/sqoop/premiers-pas-avec-sqoop/</guid>
      <description>Installation ou configuration # Sqoop est livré sous la forme d&amp;rsquo;un package binaire, mais il est composé de deux parties client et serveur distinctes. You need to install server on single node in your cluster. This node will then serve as an entry point for all connecting Sqoop clients. Server acts as a mapreduce client and therefore Hadoop must be installed and configured on machine hosting Sqoop server. Clients can be installed on any arbitrary number of machines.</description>
    </item>
    
    <item>
      <title>Importation Sqoop</title>
      <link>https://www.wikiod.com/fr/sqoop/importation-sqoop/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://www.wikiod.com/fr/sqoop/importation-sqoop/</guid>
      <description>Syntaxe # &amp;lt;rdbms-jdbc-url&amp;gt; // URL JDBC SGBDR &amp;lt;username&amp;gt; // Nom d&amp;rsquo;utilisateur de la base de données RDBMS &amp;lt;password&amp;gt; // Mot de passe de la base de données RDBMS &amp;lt;table-name&amp;gt; // table de base de données RDBMS &amp;lt;hdfs-home-dir&amp;gt; // Répertoire d&amp;rsquo;accueil HDFS &amp;lt;condition&amp;gt; // Condition pouvant être exprimée sous la forme d&amp;rsquo;une requête SQL avec une clause WHERE. &amp;lt;requête-sql&amp;gt; // Requête SQL &amp;lt;target-dir&amp;gt; // Répertoire cible HDFS Sqoop est un outil de ligne de commande Hadoop qui importe une table d&amp;rsquo;une source de données RDBMS vers HDFS et vice versa.</description>
    </item>
    
    <item>
      <title>Connecter Sqoop à d&#39;autres bases de donnéesdatastores</title>
      <link>https://www.wikiod.com/fr/sqoop/connecter-sqoop-a-dautres-bases-de-donneesdatastores/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://www.wikiod.com/fr/sqoop/connecter-sqoop-a-dautres-bases-de-donneesdatastores/</guid>
      <description>Montre comment un script sqoop peut être utilisé pour importer des données à partir de divers magasins de données/bases de données.
Charger le pilote JDBC # Pour accéder à la base de données MS SQL Server, Sqoop nécessite un pilote JDBC supplémentaire qui peut être téléchargé auprès de Microsoft. Les étapes suivantes installeront le pilote MSSQL Server JDBC sur Sqoop :
wget &#39;http://download.microsoft.com/download/0/2/A/02AAE597-3865-456C-AE7F-613F99F850A8/sqljdbc_4.0.2206.100_enu.tar.gz&#39; tar -xvzf sqljdbc_4 cp sqljdbc_4.0/enu/sqljdbc4.jar /usr/hdp/current/sqoop-server/lib/ Valider la connexion # Pour vérifier que la connexion au serveur est valide :</description>
    </item>
    
    <item>
      <title>Exportation Sqoop</title>
      <link>https://www.wikiod.com/fr/sqoop/exportation-sqoop/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://www.wikiod.com/fr/sqoop/exportation-sqoop/</guid>
      <description>Exemple de base d&amp;rsquo;exportation Sqoop # L&amp;rsquo;outil d&amp;rsquo;exportation exporte un ensemble de fichiers de HDFS vers un RDBMS. La table cible doit déjà exister dans la base de données. Les fichiers d&amp;rsquo;entrée sont lus et analysés dans un ensemble d&amp;rsquo;enregistrements selon les délimiteurs spécifiés par l&amp;rsquo;utilisateur.
Exemple :
sqoop export \ --connect=&amp;quot;jdbc:&amp;lt;databaseconnector&amp;gt;&amp;quot; \ --username=&amp;lt;username&amp;gt; \ --password=&amp;lt;password&amp;gt; \ --export-dir=&amp;lt;hdfs export directory&amp;gt; \ --table=&amp;lt;tablename&amp;gt; </description>
    </item>
    
    <item>
      <title>fusionner des ensembles de données importés via une importation incrémentielle à l&#39;aide de Sqoop</title>
      <link>https://www.wikiod.com/fr/sqoop/fusionner-des-ensembles-de-donnees-importes-via-une-importation-incrementielle-a-laide-de-sqoop/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://www.wikiod.com/fr/sqoop/fusionner-des-ensembles-de-donnees-importes-via-une-importation-incrementielle-a-laide-de-sqoop/</guid>
      <description>L&amp;rsquo;importation incrémentielle Sqoop entre en scène à cause d&amp;rsquo;un phénomène appelé CDC, c&amp;rsquo;est-à-dire Change Data Capture. Maintenant, qu&amp;rsquo;est-ce que CDC ?
CDC est un modèle de conception qui capture les modifications de données individuelles au lieu de traiter l&amp;rsquo;ensemble des données. Au lieu de vider notre base de données entière, en utilisant CDC, nous pourrions capturer uniquement les modifications de données apportées à la base de données principale.
Par exemple : si nous sommes confrontés à un problème de données, par exemple, 1 lakh d&amp;rsquo;entrées de données entrant quotidiennement dans le SGBDR et que nous devons obtenir ces données dans Hadoop quotidiennement, nous voudrions simplement obtenir les données nouvellement ajoutées, car l&amp;rsquo;importation les données RDBMS complètes quotidiennement à Hadoop seront une surcharge et retarderont également la disponibilité des données.</description>
    </item>
    
  </channel>
</rss>
