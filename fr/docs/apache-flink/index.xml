<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Tutoriel apache-flink on </title>
    <link>https://www.wikiod.com/fr/docs/apache-flink/</link>
    <description>Recent content in Tutoriel apache-flink on </description>
    <generator>Hugo -- gohugo.io</generator><atom:link href="https://www.wikiod.com/fr/docs/apache-flink/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Premiers pas avec apache-flink</title>
      <link>https://www.wikiod.com/fr/apache-flink/premiers-pas-avec-apache-flink/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://www.wikiod.com/fr/apache-flink/premiers-pas-avec-apache-flink/</guid>
      <description>Présentation et exigences # Qu&amp;rsquo;est-ce que Flink # Comme Apache Hadoop et Apache Spark, Apache Flink est un framework open source piloté par la communauté pour l&amp;rsquo;analyse Big Data distribuée. . Écrit en Java, Flink possède des API pour Scala, Java et Python, permettant des analyses de flux par lots et en temps réel.
Conditions # un environnement de type UNIX, tel que Linux, Mac OS X ou Cygwin ; Java 6.</description>
    </item>
    
    <item>
      <title>Consommer les données de Kafka</title>
      <link>https://www.wikiod.com/fr/apache-flink/consommer-les-donnees-de-kafka/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://www.wikiod.com/fr/apache-flink/consommer-les-donnees-de-kafka/</guid>
      <description>Schémas de désérialisation intégrés # SimpleStringSchema : SimpleStringSchema désérialise le message sous forme de chaîne. Dans le cas où vos messages auraient des clés, ces dernières seront ignorées.
new FlinkKafkaConsumer09&amp;lt;&amp;gt;(kafkaInputTopic, new SimpleStringSchema(), prop); JSONDeserializationSchema
JSONDeserializationSchema désérialise les messages au format json à l&amp;rsquo;aide de jackson et renvoie un flux d&amp;rsquo;objets com.fasterxml.jackson.databind.node.ObjectNode. Vous pouvez ensuite utiliser la méthode .get(&amp;quot;property&amp;quot;) pour accéder aux champs. Encore une fois, les clés sont ignorées.
new FlinkKafkaConsumer09&amp;lt;&amp;gt;(kafkaInputTopic, new JSONDeserializationSchema(), prop); JSONKeyValueDeserializationSchema</description>
    </item>
    
    <item>
      <title>Points de sauvegarde et points de contrôle externalisés</title>
      <link>https://www.wikiod.com/fr/apache-flink/points-de-sauvegarde-et-points-de-controle-externalises/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://www.wikiod.com/fr/apache-flink/points-de-sauvegarde-et-points-de-controle-externalises/</guid>
      <description>Les points de sauvegarde sont des points de contrôle &amp;ldquo;fat&amp;rdquo;, stockés en externe qui nous permettent de reprendre un programme flink avec état après un échec permanent, une annulation ou une mise à jour du code. Avant Flink 1.2 et l&amp;rsquo;introduction des points de contrôle externalisés, les points de sauvegarde devaient être déclenchés explicitement.
Points de contrôle externalisés (Flink 1.2+) # Avant la version 1.2, le seul moyen de persister dans l&amp;rsquo;état/de conserver un point de contrôle après l&amp;rsquo;arrêt/l&amp;rsquo;annulation/l&amp;rsquo;échec persistant d&amp;rsquo;un travail était via un point de sauvegarde, qui est déclenché manuellement.</description>
    </item>
    
    <item>
      <title>enregistrement</title>
      <link>https://www.wikiod.com/fr/apache-flink/enregistrement/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://www.wikiod.com/fr/apache-flink/enregistrement/</guid>
      <description>Cette rubrique montre comment utiliser et configurer la journalisation (log4j) dans les applications Flink.
Configuration de la journalisation # Mode local # En mode local, par exemple lors de l&amp;rsquo;exécution de votre application à partir d&amp;rsquo;un IDE, vous pouvez configurer log4j comme d&amp;rsquo;habitude, c&amp;rsquo;est-à-dire en rendant disponible un log4j.properties dans le classpath. Un moyen simple dans maven est de créer log4j.properties dans le dossier src/main/resources. Voici un exemple:
log4j.rootLogger=INFO, console # patterns: # d = date # c = class # F = file # p = priority (INFO, WARN, etc) # x = NDC (nested diagnostic context) associated with the thread that generated the logging event # m = message # Log all infos in the console log4j.</description>
    </item>
    
    <item>
      <title>Point de contrôle</title>
      <link>https://www.wikiod.com/fr/apache-flink/point-de-controle/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://www.wikiod.com/fr/apache-flink/point-de-controle/</guid>
      <description>(testé sur Flink 1.2 et inférieur)
Chaque fonction, source ou opérateur de Flink peut être avec état. Les points de contrôle permettent à Flink de récupérer l&amp;rsquo;état et les positions dans les flux pour donner à l&amp;rsquo;application la même sémantique qu&amp;rsquo;une exécution sans échec. C&amp;rsquo;est le mécanisme derrière les garanties de tolérance aux pannes et de traitement exactly-once.
Lisez cet article pour comprendre les éléments internes.
Les points de contrôle ne sont utiles qu&amp;rsquo;en cas d&amp;rsquo;échec dans le cluster, par exemple lorsqu&amp;rsquo;un gestionnaire de tâches échoue.</description>
    </item>
    
    <item>
      <title>Comment définir un schéma de (dé)sérialisation personnalisé</title>
      <link>https://www.wikiod.com/fr/apache-flink/comment-definir-un-schema-de-deserialisation-personnalise/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://www.wikiod.com/fr/apache-flink/comment-definir-un-schema-de-deserialisation-personnalise/</guid>
      <description>Les schémas sont utilisés par certains connecteurs (Kafka, RabbitMQ) pour transformer les messages en objets Java et vice-versa.
Exemple de schéma personnalisé # Pour utiliser un schéma personnalisé, il vous suffit d&amp;rsquo;implémenter l&amp;rsquo;une des interfaces SerializationSchema ou DeserializationSchema.
public class MyMessageSchema implements DeserializationSchema&amp;lt;MyMessage&amp;gt;, SerializationSchema&amp;lt;MyMessage&amp;gt; { @Override public MyMessage deserialize(byte[] bytes) throws IOException { return MyMessage.fromString(new String(bytes)); } @Override public byte[] serialize(MyMessage myMessage) { return myMessage.toString().getBytes(); } @Override public TypeInformation&amp;lt;MyMessage&amp;gt; getProducedType() { return TypeExtractor.</description>
    </item>
    
    <item>
      <title>API de tableau</title>
      <link>https://www.wikiod.com/fr/apache-flink/api-de-tableau/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://www.wikiod.com/fr/apache-flink/api-de-tableau/</guid>
      <description>Dépendances Maven # Pour utiliser l&amp;rsquo;API Table, ajoutez flink-table en tant que dépendance maven (en plus de flink-clients et flink-core):
&amp;lt;dependency&amp;gt; &amp;lt;groupId&amp;gt;org.apache.flink&amp;lt;/groupId&amp;gt; &amp;lt;artifactId&amp;gt;flink-table_2.11&amp;lt;/artifactId&amp;gt; &amp;lt;version&amp;gt;1.1.4&amp;lt;/version&amp;gt; &amp;lt;/dependency&amp;gt; Assurez-vous que la version scala (ici 2.11) est compatible avec votre système.
Agrégation simple à partir d&amp;rsquo;un CSV # Étant donné le fichier CSV peoples.csv :
1,Reed,United States,Female 2,Bradley,United States,Female 3,Adams,United States,Male 4,Lane,United States,Male 5,Marshall,United States,Female 6,Garza,United States,Male 7,Gutierrez,United States,Male 8,Fox,Germany,Female 9,Medina,United States,Male 10,Nichols,United States,Male 11,Woods,United States,Male 12,Welch,United States,Female 13,Burke,United States,Female 14,Russell,United States,Female 15,Burton,United States,Male 16,Johnson,United States,Female 17,Flores,United States,Male 18,Boyd,United States,Male 19,Evans,Germany,Male 20,Stephens,United States,Male Nous voulons compter les personnes par pays et par pays+genre :</description>
    </item>
    
  </channel>
</rss>
