<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Tutoriel scikit-learn on </title>
    <link>https://www.wikiod.com/fr/docs/scikit-learn/</link>
    <description>Recent content in Tutoriel scikit-learn on </description>
    <generator>Hugo -- gohugo.io</generator><atom:link href="https://www.wikiod.com/fr/docs/scikit-learn/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Premiers pas avec scikit-learn</title>
      <link>https://www.wikiod.com/fr/scikit-learn/premiers-pas-avec-scikit-learn/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://www.wikiod.com/fr/scikit-learn/premiers-pas-avec-scikit-learn/</guid>
      <description>Installation de scikit-learn # La version stable actuelle de scikit-learn [nécessite][3] :
Python (&amp;gt;= 2.6 ou &amp;gt;= 3.3), NumPy (&amp;gt;= 1.6.1), SciPy (&amp;gt;= 0,9). Pour la plupart des installations, le gestionnaire de packages python pip peut installer python et toutes ses dépendances :
pip install scikit-learn Cependant, pour les systèmes Linux, il est recommandé d&amp;rsquo;utiliser le gestionnaire de packages &amp;ldquo;conda&amp;rdquo; pour éviter d&amp;rsquo;éventuels processus de construction.
conda install scikit-learn Pour vérifier que vous avez scikit-learn, exécutez en shell :</description>
    </item>
    
    <item>
      <title>Sélection du modèle</title>
      <link>https://www.wikiod.com/fr/scikit-learn/selection-du-modele/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://www.wikiod.com/fr/scikit-learn/selection-du-modele/</guid>
      <description>Validation croisée # Apprendre les paramètres d&amp;rsquo;une fonction de prédiction et la tester sur les mêmes données est une erreur méthodologique : un modèle qui se contenterait de répéter les étiquettes des échantillons qu&amp;rsquo;il vient de voir aurait un score parfait mais ne parviendrait pas à prédire quoi que ce soit d&amp;rsquo;utile sur le moment- données invisibles. Cette situation est appelée surajustement. Pour l&amp;rsquo;éviter, il est courant lors de la réalisation d&amp;rsquo;une expérience d&amp;rsquo;apprentissage automatique (supervisé) de conserver une partie des données disponibles sous la forme d&amp;rsquo;un ensemble de tests X_test, y_test.</description>
    </item>
    
    <item>
      <title>Classification</title>
      <link>https://www.wikiod.com/fr/scikit-learn/classification/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://www.wikiod.com/fr/scikit-learn/classification/</guid>
      <description>RandomForestClassifier # Une forêt aléatoire est un méta estimateur qui adapte un certain nombre de classificateurs d&amp;rsquo;arbre de décision sur divers sous-échantillons de l&amp;rsquo;ensemble de données et utilise la moyenne pour améliorer la précision prédictive et contrôler le surajustement.
Un exemple d&amp;rsquo;utilisation simple :
Importer:
from sklearn.ensemble import RandomForestClassifier Définir les données de train et les données cibles :
train = [[1,2,3],[2,5,1],[2,1,7]] target = [0,1,0] Les valeurs dans target représentent l&amp;rsquo;étiquette que vous souhaitez prédire.</description>
    </item>
    
    <item>
      <title>Sélection de fonctionnalité</title>
      <link>https://www.wikiod.com/fr/scikit-learn/selection-de-fonctionnalite/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://www.wikiod.com/fr/scikit-learn/selection-de-fonctionnalite/</guid>
      <description>Suppression des fonctionnalités à faible variance # Il s&amp;rsquo;agit d&amp;rsquo;une technique de sélection de caractéristiques très basique.
Son idée sous-jacente est que si une caractéristique est constante (c&amp;rsquo;est-à-dire qu&amp;rsquo;elle a une variance de 0), elle ne peut pas être utilisée pour trouver des modèles intéressants et peut être supprimée de l&amp;rsquo;ensemble de données.
Par conséquent, une approche heuristique de l&amp;rsquo;élimination des caractéristiques consiste à supprimer d&amp;rsquo;abord toutes les caractéristiques dont la variance est inférieure à un seuil (faible).</description>
    </item>
    
    <item>
      <title>Régression</title>
      <link>https://www.wikiod.com/fr/scikit-learn/regression/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://www.wikiod.com/fr/scikit-learn/regression/</guid>
      <description>Moindres carrés ordinaires # [Moindres carrés ordinaires] (https://en.wikipedia.org/wiki/Ordinary_least_squares) est une méthode pour trouver la combinaison linéaire de caractéristiques qui correspond le mieux au résultat observé dans le sens suivant.
Si le vecteur des résultats à prédire est y et que les variables explicatives forment la matrice X, alors OLS trouvera le vecteur β résolvant
minβ|y^ - y|22,
où y^ = X β est la prédiction linéaire.
Dans sklearn, cela se fait en utilisant sklearn.</description>
    </item>
    
    <item>
      <title>Réduction de la dimensionnalité (sélection des fonctionnalités)</title>
      <link>https://www.wikiod.com/fr/scikit-learn/reduction-de-la-dimensionnalite-selection-des-fonctionnalites/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://www.wikiod.com/fr/scikit-learn/reduction-de-la-dimensionnalite-selection-des-fonctionnalites/</guid>
      <description>Réduire la dimension avec l&amp;rsquo;analyse en composantes principales # Analyse en composantes principales trouve des séquences de combinaisons linéaires des caractéristiques. La première combinaison linéaire maximise la variance des caractéristiques (sous réserve d&amp;rsquo;une contrainte d&amp;rsquo;unité). Chacune des combinaisons linéaires suivantes maximise la variance des caractéristiques dans le sous-espace orthogonal à celui couvert par les combinaisons linéaires précédentes.
Une technique courante de réduction de dimension consiste à n&amp;rsquo;utiliser que les k premières de ces combinaisons linéaires.</description>
    </item>
    
    <item>
      <title>Caractéristique de fonctionnement du récepteur (ROC)</title>
      <link>https://www.wikiod.com/fr/scikit-learn/caracteristique-de-fonctionnement-du-recepteur-roc/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://www.wikiod.com/fr/scikit-learn/caracteristique-de-fonctionnement-du-recepteur-roc/</guid>
      <description>Introduction au ROC et à l&amp;rsquo;AUC # Exemple de métrique de caractéristique de fonctionnement du récepteur (ROC) pour évaluer la qualité de sortie du classificateur.
Les courbes ROC présentent généralement un taux de vrais positifs sur l&amp;rsquo;axe Y et un taux de faux positifs sur l&amp;rsquo;axe X. Cela signifie que le coin supérieur gauche du graphique est le point &amp;ldquo;idéal&amp;rdquo; - un taux de faux positifs de zéro et un taux de vrais positifs de un.</description>
    </item>
    
  </channel>
</rss>
