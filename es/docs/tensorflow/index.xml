<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Tutorial de tensorflow on </title>
    <link>https://www.wikiod.com/es/docs/tensorflow/</link>
    <description>Recent content in Tutorial de tensorflow on </description>
    <generator>Hugo -- gohugo.io</generator><atom:link href="https://www.wikiod.com/es/docs/tensorflow/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Primeros pasos con tensorflow</title>
      <link>https://www.wikiod.com/es/tensorflow/primeros-pasos-con-tensorflow/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://www.wikiod.com/es/tensorflow/primeros-pasos-con-tensorflow/</guid>
      <description>Ejemplo básico # Tensorflow es más que un marco de aprendizaje profundo. Es un marco de cálculo general para realizar operaciones matemáticas generales de manera paralela y distribuida. Un ejemplo de esto se describe a continuación.
Regresión lineal # Un ejemplo estadístico básico que se utiliza comúnmente y es bastante simple de calcular es ajustar una línea a un conjunto de datos. El método para hacerlo en tensorflow se describe a continuación en el código y los comentarios.</description>
    </item>
    
    <item>
      <title>Usando convolución 1D</title>
      <link>https://www.wikiod.com/es/tensorflow/usando-convolucion-1d/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://www.wikiod.com/es/tensorflow/usando-convolucion-1d/</guid>
      <description>Ejemplo básico # Actualización: TensorFlow ahora es compatible con la convolución 1D desde la versión r0.11, usando [tf.nn.conv1d](https://www.tensorflow.org/versions/r0.11/api_docs/python/nn. html#conv1d).
Considere un ejemplo básico con una entrada de longitud &amp;ldquo;10&amp;rdquo; y dimensión &amp;ldquo;16&amp;rdquo;. El tamaño del lote es 32. Por lo tanto, tenemos un marcador de posición con la forma de entrada [batch_size, 10, 16].
batch_size = 32 x = tf.placeholder(tf.float32, [batch_size, 10, 16]) Luego creamos un filtro con ancho 3, y tomamos 16 canales como entrada, y también sacamos 16 canales.</description>
    </item>
    
    <item>
      <title>¿Cómo usar las colecciones de gráficos de TensorFlow?</title>
      <link>https://www.wikiod.com/es/tensorflow/como-usar-las-colecciones-de-graficos-de-tensorflow/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://www.wikiod.com/es/tensorflow/como-usar-las-colecciones-de-graficos-de-tensorflow/</guid>
      <description>Cuando tiene un modelo enorme, es útil formar algunos grupos de tensores en su gráfico computacional, que están conectados entre sí. Por ejemplo, la clase tf.GraphKeys contiene colecciones estándar como:
tf.GraphKeys.VARIABLES tf.GraphKeys.TRAINABLE_VARIABLES tf.GraphKeys.SUMMARIES Crea tu propia colección y úsala para recolectar todas tus pérdidas. # Aquí crearemos una colección para las pérdidas del gráfico computacional de Neural Network.
Primero cree un gráfico computacional así:
with tf.variable_scope(&amp;quot;Layer&amp;quot;): W = tf.get_variable(&amp;quot;weights&amp;quot;, [m, k], initializer=tf.</description>
    </item>
    
    <item>
      <title>Guardar y restaurar un modelo en TensorFlow</title>
      <link>https://www.wikiod.com/es/tensorflow/guardar-y-restaurar-un-modelo-en-tensorflow/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://www.wikiod.com/es/tensorflow/guardar-y-restaurar-un-modelo-en-tensorflow/</guid>
      <description>Tensorflow distingue entre guardar/restaurar los valores actuales de todas las variables en un gráfico y guardar/restaurar la estructura del gráfico real. Para restaurar el gráfico, puede usar las funciones de Tensorflow o simplemente volver a llamar a su pieza de código, que creó el gráfico en primer lugar. Al definir el gráfico, también debe pensar en qué y cómo las variables/operaciones deben recuperarse una vez que el gráfico se haya guardado y restaurado.</description>
    </item>
    
    <item>
      <title>Creación de RNN, LSTM y RNNLSTM bidireccionales con TensorFlow</title>
      <link>https://www.wikiod.com/es/tensorflow/creacion-de-rnn-lstm-y-rnnlstm-bidireccionales-con-tensorflow/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://www.wikiod.com/es/tensorflow/creacion-de-rnn-lstm-y-rnnlstm-bidireccionales-con-tensorflow/</guid>
      <description>Creando un LSTM bidireccional # import tensorflow as tf dims, layers = 32, 2 # Creating the forward and backwards cells lstm_fw_cell = tf.nn.rnn_cell.BasicLSTMCell(dims, forget_bias=1.0) lstm_bw_cell = tf.nn.rnn_cell.BasicLSTMCell(dims, forget_bias=1.0) # Pass lstm_fw_cell / lstm_bw_cell directly to tf.nn.bidrectional_rnn # if only a single layer is needed lstm_fw_multicell = tf.nn.rnn_cell.MultiRNNCell([lstm_fw_cell]*layers) lstm_bw_multicell = tf.nn.rnn_cell.MultiRNNCell([lstm_bw_cell]*layers) # tf.nn.bidirectional_rnn takes a list of tensors with shape # [batch_size x cell_fw.state_size], so separate the input into discrete # timesteps.</description>
    </item>
    
    <item>
      <title>Uso de la normalización por lotes</title>
      <link>https://www.wikiod.com/es/tensorflow/uso-de-la-normalizacion-por-lotes/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://www.wikiod.com/es/tensorflow/uso-de-la-normalizacion-por-lotes/</guid>
      <description>Parámetros # contrib.layers.batch_norm params Comentarios beta tipo bool de python. Si centrar o no moving_mean y moving_variance &amp;mdash;&amp;mdash; &amp;mdash;&amp;mdash; gamma tipo bool de python. Si escalar o no moving_mean y moving_variance &amp;mdash;&amp;mdash; &amp;mdash;&amp;mdash; es_entrenamiento Acepta python bool o TensorFlow tf.palceholder(tf.bool) &amp;mdash;&amp;mdash; &amp;mdash;&amp;mdash; decadencia La configuración predeterminada es decay=0.999. Un valor más pequeño (es decir, decay=0.9) es mejor para conjuntos de datos más pequeños y/o menos pasos de entrenamiento. Aquí hay una captura de pantalla del resultado del ejemplo de trabajo anterior.</description>
    </item>
    
    <item>
      <title>Usando la condición if dentro del gráfico de TensorFlow con tf.cond</title>
      <link>https://www.wikiod.com/es/tensorflow/usando-la-condicion-if-dentro-del-grafico-de-tensorflow-con-tfcond/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://www.wikiod.com/es/tensorflow/usando-la-condicion-if-dentro-del-grafico-de-tensorflow-con-tfcond/</guid>
      <description>Parámetros # Parámetro Detalles prensa un tensor TensorFlow de tipo bool fn1 una función invocable, sin argumento fn2 una función invocable, sin argumento nombre (opcional) nombre de la operación pred no puede ser simplemente Verdadero o Falso, necesita ser un Tensor Las funciones fn1 y fn2 deberían devolver el mismo número de salidas, con los mismos tipos. Ejemplo básico # x = tf.constant(1.) bool = tf.constant(True) res = tf.cond(bool, lambda: tf.</description>
    </item>
    
    <item>
      <title>Cómo depurar una pérdida de memoria en TensorFlow</title>
      <link>https://www.wikiod.com/es/tensorflow/como-depurar-una-perdida-de-memoria-en-tensorflow/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://www.wikiod.com/es/tensorflow/como-depurar-una-perdida-de-memoria-en-tensorflow/</guid>
      <description>Use Graph.finalize() para capturar los nodos que se agregan al gráfico # El modo más común de usar TensorFlow involucra primero construir un gráfico de flujo de datos de los operadores de TensorFlow (como tf.constant() y tf.matmul(), luego ejecutar pasos llamando al tf.Session.run() método en un bucle (por ejemplo, un bucle de entrenamiento).
Una fuente común de fugas de memoria es cuando el ciclo de entrenamiento contiene llamadas que agregan nodos al gráfico, y estos se ejecutan en cada iteración, lo que hace que el gráfico crezca.</description>
    </item>
    
    <item>
      <title>Medir el tiempo de ejecución de operaciones individuales</title>
      <link>https://www.wikiod.com/es/tensorflow/medir-el-tiempo-de-ejecucion-de-operaciones-individuales/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://www.wikiod.com/es/tensorflow/medir-el-tiempo-de-ejecucion-de-operaciones-individuales/</guid>
      <description>Ejemplo básico con el objeto Timeline de TensorFlow # El Timeline object te permite obtener el tiempo de ejecución de cada nodo en el gráfico:
usas un sess.run() clásico pero también especificas los argumentos opcionales options y run_metadata luego creas un objeto Timeline con los datos run_metadata.step_stats Aquí hay un programa de ejemplo que mide el rendimiento de una multiplicación de matrices:
import tensorflow as tf from tensorflow.python.client import timeline x = tf.</description>
    </item>
    
    <item>
      <title>leyendo los datos</title>
      <link>https://www.wikiod.com/es/tensorflow/leyendo-los-datos/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://www.wikiod.com/es/tensorflow/leyendo-los-datos/</guid>
      <description>Cómo cargar imágenes y etiquetas desde un archivo TXT # No se ha explicado en la documentación de Tensorflow cómo cargar imágenes y etiquetas directamente desde un archivo TXT. El siguiente código ilustra cómo lo logré. Sin embargo, eso no quiere decir que sea la mejor manera de hacerlo y que de esta manera ayudará en pasos posteriores.
Por ejemplo, estoy cargando las etiquetas en un solo valor entero {0,1} mientras que la documentación usa un vector único [0,1].</description>
    </item>
    
  </channel>
</rss>
