<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Tutorial de scikit-learn on </title>
    <link>https://www.wikiod.com/es/docs/scikit-learn/</link>
    <description>Recent content in Tutorial de scikit-learn on </description>
    <generator>Hugo -- gohugo.io</generator><atom:link href="https://www.wikiod.com/es/docs/scikit-learn/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Primeros pasos con scikit-learn</title>
      <link>https://www.wikiod.com/es/scikit-learn/primeros-pasos-con-scikit-learn/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://www.wikiod.com/es/scikit-learn/primeros-pasos-con-scikit-learn/</guid>
      <description>Instalación de scikit-learn # La versión estable actual de scikit-learn requiere:
Python (&amp;gt;= 2.6 o &amp;gt;= 3.3), NúmPy (&amp;gt;= 1.6.1), SciPy (&amp;gt;= 0.9). Para la mayoría de las instalaciones, el administrador de paquetes de python pip puede instalar python y todas sus dependencias:
pip install scikit-learn Sin embargo, para los sistemas Linux, se recomienda utilizar el administrador de paquetes conda para evitar posibles procesos de compilación.
conda install scikit-learn Para verificar que tiene scikit-learn, ejecute en shell:</description>
    </item>
    
    <item>
      <title>Selección de modelo</title>
      <link>https://www.wikiod.com/es/scikit-learn/seleccion-de-modelo/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://www.wikiod.com/es/scikit-learn/seleccion-de-modelo/</guid>
      <description>Validación cruzada # Aprender los parámetros de una función de predicción y probarla con los mismos datos es un error metodológico: un modelo que simplemente repita las etiquetas de las muestras que acaba de ver tendría una puntuación perfecta pero no podría predecir nada útil en el momento. datos no vistos. Esta situación se denomina sobreajuste. Para evitarlo, es una práctica común cuando se realiza un experimento de aprendizaje automático (supervisado) mantener parte de los datos disponibles como un conjunto de prueba X_test, y_test.</description>
    </item>
    
    <item>
      <title>Clasificación</title>
      <link>https://www.wikiod.com/es/scikit-learn/clasificacion/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://www.wikiod.com/es/scikit-learn/clasificacion/</guid>
      <description>ClasificadorBosqueAleatorio # Un bosque aleatorio es un metaestimador que ajusta una serie de clasificadores de árboles de decisión en varias submuestras del conjunto de datos y utiliza el promedio para mejorar la precisión predictiva y controlar el sobreajuste.
Un ejemplo de uso simple:
Importar:
from sklearn.ensemble import RandomForestClassifier Defina los datos del tren y los datos del objetivo:
train = [[1,2,3],[2,5,1],[2,1,7]] target = [0,1,0] Los valores en objetivo representan la etiqueta que desea predecir.</description>
    </item>
    
    <item>
      <title>Selección de características</title>
      <link>https://www.wikiod.com/es/scikit-learn/seleccion-de-caracteristicas/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://www.wikiod.com/es/scikit-learn/seleccion-de-caracteristicas/</guid>
      <description>Eliminación de características de baja varianza # Esta es una técnica de selección de características muy básica.
Su idea subyacente es que si una característica es constante (es decir, tiene una varianza de 0), entonces no se puede usar para encontrar patrones interesantes y se puede eliminar del conjunto de datos.
En consecuencia, un enfoque heurístico para la eliminación de características consiste en eliminar primero todas las características cuya varianza está por debajo de algún umbral (bajo).</description>
    </item>
    
    <item>
      <title>Regresión</title>
      <link>https://www.wikiod.com/es/scikit-learn/regresion/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://www.wikiod.com/es/scikit-learn/regresion/</guid>
      <description>Mínimos cuadrados ordinarios # [Mínimos cuadrados ordinarios] (https://en.wikipedia.org/wiki/Ordinary_least_squares) es un método para encontrar la combinación lineal de características que mejor se ajuste al resultado observado en el siguiente sentido.
Si el vector de resultados a predecir es y, y las variables explicativas forman la matriz X, entonces OLS encontrará el vector β resolviendo
minβ|y^ - y|22,
donde y^ = X β es la predicción lineal.
En sklearn, esto se hace usando sklearn.</description>
    </item>
    
    <item>
      <title>Reducción de dimensionalidad (selección de características)</title>
      <link>https://www.wikiod.com/es/scikit-learn/reduccion-de-dimensionalidad-seleccion-de-caracteristicas/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://www.wikiod.com/es/scikit-learn/reduccion-de-dimensionalidad-seleccion-de-caracteristicas/</guid>
      <description>Reduciendo la dimensión con análisis de componentes principales # Análisis de componentes principales encuentra secuencias de combinaciones lineales de las características. La primera combinación lineal maximiza la varianza de las características (sujeto a una restricción de unidad). Cada una de las siguientes combinaciones lineales maximiza la varianza de las características en el subespacio ortogonal al abarcado por las combinaciones lineales anteriores.
Una técnica común de reducción de dimensiones es usar solo las k primeras combinaciones lineales de este tipo.</description>
    </item>
    
    <item>
      <title>Característica operativa del receptor (ROC)</title>
      <link>https://www.wikiod.com/es/scikit-learn/caracteristica-operativa-del-receptor-roc/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://www.wikiod.com/es/scikit-learn/caracteristica-operativa-del-receptor-roc/</guid>
      <description>Introducción a ROC y AUC # Ejemplo de métrica de características operativas del receptor (ROC) para evaluar la calidad de salida del clasificador.
Las curvas ROC suelen presentar una tasa de verdaderos positivos en el eje Y y una tasa de falsos positivos en el eje X. Esto significa que la esquina superior izquierda de la gráfica es el punto &amp;ldquo;ideal&amp;rdquo;: una tasa de falsos positivos de cero y una tasa de verdaderos positivos de uno.</description>
    </item>
    
  </channel>
</rss>
