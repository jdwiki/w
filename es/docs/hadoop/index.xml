<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Tutorial de hadoop on </title>
    <link>https://www.wikiod.com/es/docs/hadoop/</link>
    <description>Recent content in Tutorial de hadoop on </description>
    <generator>Hugo -- gohugo.io</generator><atom:link href="https://www.wikiod.com/es/docs/hadoop/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Primeros pasos con Hadoop</title>
      <link>https://www.wikiod.com/es/hadoop/primeros-pasos-con-hadoop/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://www.wikiod.com/es/hadoop/primeros-pasos-con-hadoop/</guid>
      <description>Instalación de Hadoop en ubuntu # Creación de usuario de Hadoop: # sudo addgroup hadoop Agregar un usuario: # sudo adduser --ingroup hadoop hduser001 Configuración de SSH: # su -hduser001 ssh-keygen -t rsa -P &amp;quot;&amp;quot; cat .ssh/id rsa.pub &amp;gt;&amp;gt; .ssh/authorized_keys Nota: Si recibe errores [bash: .ssh/authorized_keys: No such file or directory] mientras escribe la clave autorizada. Marque aquí.
Agregue el usuario de hadoop a la lista de sudoer: # sudo adduser hduser001 sudo Deshabilitar IPv6: # Instalación de Hadoop: # sudo add-apt-repository ppa:hadoop-ubuntu/stable sudo apt-get install hadoop Instalación o configuración en Linux # Procedimiento de configuración de un pseudo clúster distribuido</description>
    </item>
    
    <item>
      <title>Comandos de Hadoop</title>
      <link>https://www.wikiod.com/es/hadoop/comandos-de-hadoop/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://www.wikiod.com/es/hadoop/comandos-de-hadoop/</guid>
      <description>Sintaxis # Comandos Hadoop v1: hadoop fs -&amp;lt;comando&amp;gt;
Comandos Hadoop v2: hdfs dfs -&amp;lt;comando&amp;gt;
Comandos de Hadoop v1 # 1. Imprime la versión de Hadoop # hadoop version 2. Listar el contenido del directorio raíz en HDFS # # hadoop fs -ls / 3. Informe la cantidad de espacio utilizado y # disponible en el sistema de archivos montado actualmente # # hadoop fs -df hdfs:/ 4. Cuente la cantidad de directorios, archivos y bytes en # las rutas que coinciden con el patrón de archivo especificado # # hadoop fs -count hdfs:/ 5.</description>
    </item>
    
    <item>
      <title>¿Qué es HDFS?</title>
      <link>https://www.wikiod.com/es/hadoop/que-es-hdfs/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://www.wikiod.com/es/hadoop/que-es-hdfs/</guid>
      <description>Una buena explicación de HDFS y cómo funciona.
La sintaxis debe contener los comandos que se pueden usar en HDFS.
Encontrar archivos en HDFS # Para buscar un archivo en el sistema de archivos distribuido de Hadoop:
hdfs dfs -ls -R / | grep [search_term] En el comando anterior,
-ls es para listar archivos
-R es para recursivo (iterar a través de subdirectorios)
/ significa desde el directorio raíz
| para canalizar la salida del primer comando al segundo</description>
    </item>
    
    <item>
      <title>Introducción a MapReduce</title>
      <link>https://www.wikiod.com/es/hadoop/introduccion-a-mapreduce/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://www.wikiod.com/es/hadoop/introduccion-a-mapreduce/</guid>
      <description>Sintaxis # Para ejecutar el ejemplo, la sintaxis del comando es:
bin/hadoop jar hadoop-*-examples.jar wordcount [-m &amp;lt;#maps&amp;gt;] [-r &amp;lt;#reducers&amp;gt;] &amp;lt;in-dir&amp;gt; &amp;lt;out-dir&amp;gt; Para copiar datos en HDFS (desde local):
bin/hadoop dfs -mkdir &amp;lt;hdfs-dir&amp;gt; //not required in hadoop 0.17.2 and later bin/hadoop dfs -copyFromLocal &amp;lt;local-dir&amp;gt; &amp;lt;hdfs-dir&amp;gt; Programa Word Count usando MapReduce en Hadoop.
Programa de recuento de palabras (en Java y Python) # El programa de conteo de palabras es como el programa &amp;ldquo;Hello World&amp;rdquo; en MapReduce.</description>
    </item>
    
    <item>
      <title>Datos de carga de Hadoop</title>
      <link>https://www.wikiod.com/es/hadoop/datos-de-carga-de-hadoop/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://www.wikiod.com/es/hadoop/datos-de-carga-de-hadoop/</guid>
      <description>Cargar datos en hadoop hdfs # PASO 1: CREAR UN DIRECTORIO EN HDFS, CARGAR UN ARCHIVO Y LISTA DE CONTENIDOS
Aprendamos escribiendo la sintaxis. Podrá copiar y pegar los siguientes comandos de ejemplo en su terminal:
hadoop fs-mkdir: # Toma la ruta URI como argumento y crea un directorio o varios directorios.
Uso: # # hadoop fs -mkdir &amp;lt;paths&amp;gt; Ejemplo: # hadoop fs -mkdir /user/hadoop hadoop fs -mkdir /user/hadoop/dir1 /user/hadoop/dir2 /user/hadoop/dir3 hadoop fs-poner: # Copia un solo archivo src o varios archivos src del sistema de archivos local al sistema de archivos distribuido de Hadoop.</description>
    </item>
    
    <item>
      <title>matiz</title>
      <link>https://www.wikiod.com/es/hadoop/matiz/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://www.wikiod.com/es/hadoop/matiz/</guid>
      <description>Hue es una interfaz de usuario para conectarse y trabajar con la mayoría de las tecnologías Bigdata comúnmente utilizadas como HDFS, Hive, Spark, Hbase, Sqoop, Impala, Pig, Oozie, etc. Hue también admite la ejecución de consultas en bases de datos relacionales.
Hue, una aplicación web de django, se creó principalmente como un banco de trabajo para ejecutar consultas de Hive. Posteriormente, la funcionalidad de Hue aumentó para admitir diferentes componentes del ecosistema Hadoop.</description>
    </item>
    
    <item>
      <title>Depuración del código Java de Hadoop MR en el entorno de desarrollo local de Eclipse.</title>
      <link>https://www.wikiod.com/es/hadoop/depuracion-del-codigo-java-de-hadoop-mr-en-el-entorno-de-desarrollo-local-de-eclipse/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://www.wikiod.com/es/hadoop/depuracion-del-codigo-java-de-hadoop-mr-en-el-entorno-de-desarrollo-local-de-eclipse/</guid>
      <description>Lo básico que debe recordar aquí es que la depuración de un trabajo de Hadoop MR será similar a cualquier aplicación depurada de forma remota en Eclipse.
Un depurador o herramienta de depuración es un programa informático que se utiliza para probar y depurar otros programas (el programa &amp;ldquo;objetivo&amp;rdquo;). Es muy útil especialmente para un entorno Hadoop en el que hay poco margen de error y un pequeño error puede causar una gran pérdida.</description>
    </item>
    
  </channel>
</rss>
