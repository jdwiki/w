<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>tutorial do hadoop on </title>
    <link>https://www.wikiod.com/pt/docs/hadoop/</link>
    <description>Recent content in tutorial do hadoop on </description>
    <generator>Hugo -- gohugo.io</generator><atom:link href="https://www.wikiod.com/pt/docs/hadoop/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Introdução ao hadoop</title>
      <link>https://www.wikiod.com/pt/hadoop/introducao-ao-hadoop/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://www.wikiod.com/pt/hadoop/introducao-ao-hadoop/</guid>
      <description>Instalação do Hadoop no Ubuntu # Criando usuário do Hadoop: # sudo addgroup hadoop Adicionando um usuário: # sudo adduser --ingroup hadoop hduser001 Configurando o SSH: # su -hduser001 ssh-keygen -t rsa -P &amp;quot;&amp;quot; cat .ssh/id rsa.pub &amp;gt;&amp;gt; .ssh/authorized_keys Nota: Se você receber erros [bash: .ssh/authorized_keys: No such file or directory] enquanto escreve a chave autorizada. Verifique aqui.
Adicione o usuário do hadoop à lista do sudoer: # sudo adduser hduser001 sudo Desativando IPv6: # Instalando o Hadoop: # sudo add-apt-repository ppa:hadoop-ubuntu/stable sudo apt-get install hadoop Instalação ou configuração no Linux # Um procedimento de configuração de cluster pseudodistribuído</description>
    </item>
    
    <item>
      <title>Comandos do Hadoop</title>
      <link>https://www.wikiod.com/pt/hadoop/comandos-do-hadoop/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://www.wikiod.com/pt/hadoop/comandos-do-hadoop/</guid>
      <description>Sintaxe # Comandos Hadoop v1: hadoop fs -&amp;lt;comando&amp;gt;
Comandos do Hadoop v2: hdfs dfs -&amp;lt;comando&amp;gt;
Comandos do Hadoop v1 # 1. Imprima a versão do Hadoop # hadoop version 2. Liste o conteúdo do diretório raiz no HDFS # # hadoop fs -ls / 3. Relate a quantidade de espaço usado e # disponível no sistema de arquivos atualmente montado # # hadoop fs -df hdfs:/ 4. Conte o número de diretórios, arquivos e bytes em # os caminhos que correspondem ao padrão de arquivo especificado # # hadoop fs -count hdfs:/ 5.</description>
    </item>
    
    <item>
      <title>O que é HDFS?</title>
      <link>https://www.wikiod.com/pt/hadoop/o-que-e-hdfs/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://www.wikiod.com/pt/hadoop/o-que-e-hdfs/</guid>
      <description>Uma boa explicação do HDFS e como ele funciona.
A sintaxe deve conter os comandos que podem ser usados ​​no HDFS.
Encontrando arquivos no HDFS # Para localizar um arquivo no sistema de arquivos distribuído do Hadoop:
hdfs dfs -ls -R / | grep [search_term] No comando acima,
-ls é para listar arquivos
-R é recursivo (iter através de subdiretórios)
/ significa do diretório raiz
| para canalizar a saída do primeiro comando para o segundo</description>
    </item>
    
    <item>
      <title>Introdução ao MapReduce</title>
      <link>https://www.wikiod.com/pt/hadoop/introducao-ao-mapreduce/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://www.wikiod.com/pt/hadoop/introducao-ao-mapreduce/</guid>
      <description>Sintaxe # Para executar o exemplo, a sintaxe do comando é:
bin/hadoop jar hadoop-*-examples.jar wordcount [-m &amp;lt;#maps&amp;gt;] [-r &amp;lt;#reducers&amp;gt;] &amp;lt;in-dir&amp;gt; &amp;lt;out-dir&amp;gt; Para copiar dados para HDFS (do local):
bin/hadoop dfs -mkdir &amp;lt;hdfs-dir&amp;gt; //not required in hadoop 0.17.2 and later bin/hadoop dfs -copyFromLocal &amp;lt;local-dir&amp;gt; &amp;lt;hdfs-dir&amp;gt; Programa Word Count usando MapReduce no Hadoop.
Programa de contagem de palavras (em Java e Python) # O programa de contagem de palavras é como o programa &amp;ldquo;Hello World&amp;rdquo; no MapReduce.</description>
    </item>
    
    <item>
      <title>Dados de carregamento do Hadoop</title>
      <link>https://www.wikiod.com/pt/hadoop/dados-de-carregamento-do-hadoop/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://www.wikiod.com/pt/hadoop/dados-de-carregamento-do-hadoop/</guid>
      <description>Carregar dados no hadoop hdfs # PASSO 1: CRIAR UM DIRETÓRIO EM HDFS, CARREGAR UM ARQUIVO E LISTA DE CONTEÚDO
Vamos aprender escrevendo a sintaxe. Você poderá copiar e colar os seguintes comandos de exemplo em seu terminal:
hadoop fs -mkdir: # Toma os URIs do caminho como argumento e cria um diretório ou vários diretórios.
Uso: # # hadoop fs -mkdir &amp;lt;paths&amp;gt; Exemplo: # hadoop fs -mkdir /user/hadoop hadoop fs -mkdir /user/hadoop/dir1 /user/hadoop/dir2 /user/hadoop/dir3 hadoop fs -put: # Copia um único arquivo src ou vários arquivos src do sistema de arquivos local para o Hadoop Distributed File System.</description>
    </item>
    
    <item>
      <title>matiz</title>
      <link>https://www.wikiod.com/pt/hadoop/matiz/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://www.wikiod.com/pt/hadoop/matiz/</guid>
      <description>Hue é uma interface de usuário para conectar e trabalhar com a maioria das tecnologias de Bigdata comumente usadas, como HDFS, Hive, Spark, Hbase, Sqoop, Impala, Pig, Oozie etc. Hue também suporta a execução de consultas em bancos de dados relacionais.
Hue, um aplicativo da web django, foi construído principalmente como um ambiente de trabalho para executar consultas do Hive. Mais tarde, a funcionalidade do Hue aumentou para suportar diferentes componentes do Ecossistema Hadoop.</description>
    </item>
    
    <item>
      <title>Depurando o código Java do Hadoop MR no ambiente de desenvolvimento local do Eclipse.</title>
      <link>https://www.wikiod.com/pt/hadoop/depurando-o-codigo-java-do-hadoop-mr-no-ambiente-de-desenvolvimento-local-do-eclipse/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://www.wikiod.com/pt/hadoop/depurando-o-codigo-java-do-hadoop-mr-no-ambiente-de-desenvolvimento-local-do-eclipse/</guid>
      <description>A coisa básica a ser lembrada aqui é que depurar um trabalho do Hadoop MR será semelhante a qualquer aplicativo depurado remotamente no Eclipse.
Um depurador ou ferramenta de depuração é um programa de computador usado para testar e depurar outros programas (o programa “destino”). É muito útil especialmente para um ambiente Hadoop em que há pouco espaço para erros e um pequeno erro pode causar uma grande perda.
Isso é tudo o que você precisa fazer.</description>
    </item>
    
  </channel>
</rss>
