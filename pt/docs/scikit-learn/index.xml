<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Tutorial scikit-learn on </title>
    <link>https://www.wikiod.com/pt/docs/scikit-learn/</link>
    <description>Recent content in Tutorial scikit-learn on </description>
    <generator>Hugo -- gohugo.io</generator><atom:link href="https://www.wikiod.com/pt/docs/scikit-learn/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Introdução ao scikit-learn</title>
      <link>https://www.wikiod.com/pt/scikit-learn/introducao-ao-scikit-learn/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://www.wikiod.com/pt/scikit-learn/introducao-ao-scikit-learn/</guid>
      <description>Instalação do scikit-learn # A versão estável atual do scikit-learn requer:
Python (&amp;gt;= 2.6 ou &amp;gt;= 3.3), NumPy (&amp;gt;= 1.6.1), SciPy (&amp;gt;= 0,9). Para a maioria das instalações, o gerenciador de pacotes python pip pode instalar o python e todas as suas dependências:
pip install scikit-learn No entanto, para sistemas linux, é recomendado usar o gerenciador de pacotes conda para evitar possíveis processos de compilação
conda install scikit-learn Para verificar se você tem scikit-learn, execute no shell:</description>
    </item>
    
    <item>
      <title>Seleção de modelo</title>
      <link>https://www.wikiod.com/pt/scikit-learn/selecao-de-modelo/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://www.wikiod.com/pt/scikit-learn/selecao-de-modelo/</guid>
      <description>Validação cruzada # Aprender os parâmetros de uma função de previsão e testá-la nos mesmos dados é um erro metodológico: um modelo que apenas repetiria os rótulos das amostras que acabou de ver teria uma pontuação perfeita, mas não conseguiria prever nada de útil ainda. dados não vistos. Essa situação é chamada de overfitting. Para evitar isso, é uma prática comum ao realizar um experimento de aprendizado de máquina (supervisionado) manter parte dos dados disponíveis como um conjunto de teste X_test, y_test.</description>
    </item>
    
    <item>
      <title>Classificação</title>
      <link>https://www.wikiod.com/pt/scikit-learn/classificacao/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://www.wikiod.com/pt/scikit-learn/classificacao/</guid>
      <description>RandomForestClassifier # Uma floresta aleatória é um meta estimador que ajusta vários classificadores de árvore de decisão em várias subamostras do conjunto de dados e usa a média para melhorar a precisão preditiva e controlar o ajuste excessivo.
Um exemplo simples de uso:
Importar:
from sklearn.ensemble import RandomForestClassifier Defina dados de trem e dados de destino:
train = [[1,2,3],[2,5,1],[2,1,7]] target = [0,1,0] Os valores em target representam o rótulo que você deseja prever.</description>
    </item>
    
    <item>
      <title>Seleção de recursos</title>
      <link>https://www.wikiod.com/pt/scikit-learn/selecao-de-recursos/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://www.wikiod.com/pt/scikit-learn/selecao-de-recursos/</guid>
      <description>Remoção de recursos de baixa variação # Esta é uma técnica de seleção de recursos muito básica.
Sua ideia subjacente é que, se um recurso é constante (ou seja, tem 0 variância), ele não pode ser usado para encontrar padrões interessantes e pode ser removido do conjunto de dados.
Consequentemente, uma abordagem heurística para a eliminação de recursos é primeiro remover todos os recursos cuja variância está abaixo de algum limite (baixo).</description>
    </item>
    
    <item>
      <title>Regressão</title>
      <link>https://www.wikiod.com/pt/scikit-learn/regressao/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://www.wikiod.com/pt/scikit-learn/regressao/</guid>
      <description>Mínimos Quadrados Ordinários # [Ordinary Least Squares] (https://en.wikipedia.org/wiki/Ordinary_least_squares) é um método para encontrar a combinação linear de características que melhor se ajusta ao resultado observado no seguinte sentido.
Se o vetor de resultados a serem previstos for y, e as variáveis ​​explicativas formarem a matriz X, então OLS encontrará o vetor β resolvendo
minβ|y^ - y|22,
onde y^ = X β é a previsão linear.
No sklearn, isso é feito usando sklearn.</description>
    </item>
    
    <item>
      <title>Redução de dimensionalidade (seleção de recursos)</title>
      <link>https://www.wikiod.com/pt/scikit-learn/reducao-de-dimensionalidade-selecao-de-recursos/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://www.wikiod.com/pt/scikit-learn/reducao-de-dimensionalidade-selecao-de-recursos/</guid>
      <description>Reduzindo a dimensão com análise de componentes principais # Análise de componentes principais encontra sequências de combinações lineares dos recursos. A primeira combinação linear maximiza a variância dos recursos (sujeito a uma restrição de unidade). Cada uma das seguintes combinações lineares maximiza a variância das feições no subespaço ortogonal àquela abrangida pelas combinações lineares anteriores.
Uma técnica comum de redução de dimensão é usar apenas as k primeiras dessas combinações lineares.</description>
    </item>
    
    <item>
      <title>Característica Operacional do Receptor (ROC)</title>
      <link>https://www.wikiod.com/pt/scikit-learn/caracteristica-operacional-do-receptor-roc/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://www.wikiod.com/pt/scikit-learn/caracteristica-operacional-do-receptor-roc/</guid>
      <description>Introdução ao ROC e AUC # Exemplo de métrica Receiver Operating Characteristic (ROC) para avaliar a qualidade de saída do classificador.
As curvas ROC normalmente apresentam taxa de verdadeiro positivo no eixo Y e taxa de falso positivo no eixo X. Isso significa que o canto superior esquerdo do gráfico é o ponto “ideal” - uma taxa de falsos positivos de zero e uma taxa de verdadeiros positivos de um. Isso não é muito realista, mas significa que uma área maior sob a curva (AUC) geralmente é melhor.</description>
    </item>
    
  </channel>
</rss>
