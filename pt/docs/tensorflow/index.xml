<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Tutorial do tensorflow on </title>
    <link>https://www.wikiod.com/pt/docs/tensorflow/</link>
    <description>Recent content in Tutorial do tensorflow on </description>
    <generator>Hugo -- gohugo.io</generator><atom:link href="https://www.wikiod.com/pt/docs/tensorflow/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Começando com o tensorflow</title>
      <link>https://www.wikiod.com/pt/tensorflow/comecando-com-o-tensorflow/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://www.wikiod.com/pt/tensorflow/comecando-com-o-tensorflow/</guid>
      <description>Exemplo básico # O Tensorflow é mais do que apenas uma estrutura de aprendizado profundo. É uma estrutura de computação geral para realizar operações matemáticas gerais de maneira paralela e distribuída. Um exemplo de tal é descrito abaixo.
Regressão linear # Um exemplo estatístico básico que é comumente utilizado e bastante simples de calcular é ajustar uma linha a um conjunto de dados. O método para fazer isso no tensorflow é descrito abaixo no código e nos comentários.</description>
    </item>
    
    <item>
      <title>Usando convolução 1D</title>
      <link>https://www.wikiod.com/pt/tensorflow/usando-convolucao-1d/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://www.wikiod.com/pt/tensorflow/usando-convolucao-1d/</guid>
      <description>Exemplo básico # Atualização: o TensorFlow agora oferece suporte à convolução 1D desde a versão r0.11, usando [tf.nn.conv1d](https://www.tensorflow.org/versions/r0.11/api_docs/python/nn. html#conv1d).
Considere um exemplo básico com uma entrada de comprimento 10 e dimensão 16. O tamanho do lote é 32. Portanto, temos um espaço reservado com forma de entrada [batch_size, 10, 16].
batch_size = 32 x = tf.placeholder(tf.float32, [batch_size, 10, 16]) Em seguida, criamos um filtro com largura 3 e tomamos canais 16 como entrada e saída também canais 16.</description>
    </item>
    
    <item>
      <title>Como usar as coleções do TensorFlow Graph?</title>
      <link>https://www.wikiod.com/pt/tensorflow/como-usar-as-colecoes-do-tensorflow-graph/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://www.wikiod.com/pt/tensorflow/como-usar-as-colecoes-do-tensorflow-graph/</guid>
      <description>Quando você tem um modelo enorme, é útil formar alguns grupos de tensores em seu gráfico computacional, que estão conectados entre si. Por exemplo, a classe tf.GraphKeys contém coleções padrão como:
tf.GraphKeys.VARIABLES tf.GraphKeys.TRAINABLE_VARIABLES tf.GraphKeys.SUMMARIES Crie sua própria coleção e use-a para coletar todas as suas perdas. # Aqui vamos criar uma coleção para perdas do grafo computacional da Rede Neural.
Primeiro crie um gráfico computacional assim:
with tf.variable_scope(&amp;quot;Layer&amp;quot;): W = tf.</description>
    </item>
    
    <item>
      <title>Salvar e restaurar um modelo no TensorFlow</title>
      <link>https://www.wikiod.com/pt/tensorflow/salvar-e-restaurar-um-modelo-no-tensorflow/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://www.wikiod.com/pt/tensorflow/salvar-e-restaurar-um-modelo-no-tensorflow/</guid>
      <description>O Tensorflow distingue entre salvar/restaurar os valores atuais de todas as variáveis ​​em um gráfico e salvar/restaurar a estrutura real do gráfico. Para restaurar o gráfico, você pode usar as funções do Tensorflow ou apenas chamar seu código novamente, que criou o gráfico em primeiro lugar. Ao definir o gráfico, você também deve pensar em quais e como as variáveis/operações devem ser recuperadas depois que o gráfico for salvo e restaurado.</description>
    </item>
    
    <item>
      <title>Criação de RNN, LSTM e RNNLSTMs bidirecionais com o TensorFlow</title>
      <link>https://www.wikiod.com/pt/tensorflow/criacao-de-rnn-lstm-e-rnnlstms-bidirecionais-com-o-tensorflow/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://www.wikiod.com/pt/tensorflow/criacao-de-rnn-lstm-e-rnnlstms-bidirecionais-com-o-tensorflow/</guid>
      <description>Criando um LSTM bidirecional # import tensorflow as tf dims, layers = 32, 2 # Creating the forward and backwards cells lstm_fw_cell = tf.nn.rnn_cell.BasicLSTMCell(dims, forget_bias=1.0) lstm_bw_cell = tf.nn.rnn_cell.BasicLSTMCell(dims, forget_bias=1.0) # Pass lstm_fw_cell / lstm_bw_cell directly to tf.nn.bidrectional_rnn # if only a single layer is needed lstm_fw_multicell = tf.nn.rnn_cell.MultiRNNCell([lstm_fw_cell]*layers) lstm_bw_multicell = tf.nn.rnn_cell.MultiRNNCell([lstm_bw_cell]*layers) # tf.nn.bidirectional_rnn takes a list of tensors with shape # [batch_size x cell_fw.state_size], so separate the input into discrete # timesteps.</description>
    </item>
    
    <item>
      <title>Usando a normalização em lote</title>
      <link>https://www.wikiod.com/pt/tensorflow/usando-a-normalizacao-em-lote/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://www.wikiod.com/pt/tensorflow/usando-a-normalizacao-em-lote/</guid>
      <description>Parâmetros # parâmetros contrib.layers.batch_norm Observações beta tipo python bool. Se deve ou não centralizar o moving_mean e moving_variance &amp;mdash;&amp;mdash; &amp;mdash;&amp;mdash; gama tipo python bool. Se deve ou não dimensionar o moving_mean e o moving_variance &amp;mdash;&amp;mdash; &amp;mdash;&amp;mdash; is_treinamento Aceita python bool ou TensorFlow tf.palceholder(tf.bool) &amp;mdash;&amp;mdash; &amp;mdash;&amp;mdash; decadência A configuração padrão é decay=0,999. Um valor menor (ou seja, decay=0,9) é melhor para conjuntos de dados menores e/ou menos etapas de treinamento. Aqui está uma captura de tela do resultado do exemplo de trabalho acima.</description>
    </item>
    
    <item>
      <title>Usando a condição if dentro do gráfico do TensorFlow com tf.cond</title>
      <link>https://www.wikiod.com/pt/tensorflow/usando-a-condicao-if-dentro-do-grafico-do-tensorflow-com-tfcond/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://www.wikiod.com/pt/tensorflow/usando-a-condicao-if-dentro-do-grafico-do-tensorflow-com-tfcond/</guid>
      <description>Parâmetros # Parâmetro Detalhes pred um tensor TensorFlow do tipo bool fn1 uma função que pode ser chamada, sem argumento fn2 uma função que pode ser chamada, sem argumento nome (opcional) nome para a operação pred não pode ser apenas True ou False, precisa ser um tensor As funções fn1 e fn2 devem retornar o mesmo número de saídas, com os mesmos tipos. Exemplo básico # x = tf.constant(1.) bool = tf.</description>
    </item>
    
    <item>
      <title>Como depurar um vazamento de memória no TensorFlow</title>
      <link>https://www.wikiod.com/pt/tensorflow/como-depurar-um-vazamento-de-memoria-no-tensorflow/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://www.wikiod.com/pt/tensorflow/como-depurar-um-vazamento-de-memoria-no-tensorflow/</guid>
      <description>Use Graph.finalize() para capturar nós sendo adicionados ao gráfico # O modo mais comum de usar o TensorFlow envolve primeiro criar um gráfico de fluxo de dados de operadores do TensorFlow (como tf.constant() e tf.matmul(), depois executar etapas chamando o método tf.Session.run() em um loop (por exemplo, um loop de treinamento).
Uma fonte comum de vazamentos de memória é onde o loop de treinamento contém chamadas que adicionam nós ao gráfico e são executadas em cada iteração, fazendo com que o gráfico cresça.</description>
    </item>
    
    <item>
      <title>Meça o tempo de execução de operações individuais</title>
      <link>https://www.wikiod.com/pt/tensorflow/meca-o-tempo-de-execucao-de-operacoes-individuais/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://www.wikiod.com/pt/tensorflow/meca-o-tempo-de-execucao-de-operacoes-individuais/</guid>
      <description>Exemplo básico com o objeto Timeline do TensorFlow # O objeto Timeline permite obter o tempo de execução de cada nó no gráfico:
você usa um sess.run() clássico, mas também especifica os argumentos opcionais options e run_metadata você então cria um objeto Timeline com os dados run_metadata.step_stats Aqui está um exemplo de programa que mede o desempenho de uma multiplicação de matrizes:
import tensorflow as tf from tensorflow.python.client import timeline x = tf.</description>
    </item>
    
    <item>
      <title>Lendo os dados</title>
      <link>https://www.wikiod.com/pt/tensorflow/lendo-os-dados/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://www.wikiod.com/pt/tensorflow/lendo-os-dados/</guid>
      <description>Como carregar imagens e rótulos de um arquivo TXT # Não foi explicado na documentação do Tensorflow como carregar imagens e rótulos diretamente de um arquivo TXT. O código abaixo ilustra como consegui. No entanto, isso não significa que seja a melhor maneira de fazê-lo e que essa forma ajudará em etapas posteriores.
Por exemplo, estou carregando os rótulos em um único valor inteiro {0,1} enquanto a documentação usa um vetor one-hot [0,1].</description>
    </item>
    
  </channel>
</rss>
