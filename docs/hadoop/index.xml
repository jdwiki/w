<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>hadoop Tutorial on </title>
    <link>https://www.wikiod.com/docs/hadoop/</link>
    <description>Recent content in hadoop Tutorial on </description>
    <generator>Hugo -- gohugo.io</generator><atom:link href="https://www.wikiod.com/docs/hadoop/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Getting started with hadoop</title>
      <link>https://www.wikiod.com/hadoop/getting-started-with-hadoop/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://www.wikiod.com/hadoop/getting-started-with-hadoop/</guid>
      <description>Installation of Hadoop on ubuntu # Creating Hadoop User: # sudo addgroup hadoop Adding a user: # sudo adduser --ingroup hadoop hduser001 Configuring SSH: # su -hduser001 ssh-keygen -t rsa -P &amp;quot;&amp;quot; cat .ssh/id rsa.pub &amp;gt;&amp;gt; .ssh/authorized_keys Note: If you get errors [bash: .ssh/authorized_keys: No such file or directory] whilst writing the authorized key. Check here.
Add hadoop user to sudoer&amp;rsquo;s list: # sudo adduser hduser001 sudo Disabling IPv6: # Installing Hadoop: # sudo add-apt-repository ppa:hadoop-ubuntu/stable sudo apt-get install hadoop Installation or Setup on Linux # A Pseudo Distributed Cluster Setup Procedure</description>
    </item>
    
    <item>
      <title>Hadoop commands</title>
      <link>https://www.wikiod.com/hadoop/hadoop-commands/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://www.wikiod.com/hadoop/hadoop-commands/</guid>
      <description>Syntax # Hadoop v1 commands: hadoop fs -&amp;lt;command&amp;gt;
Hadoop v2 commands: hdfs dfs -&amp;lt;command&amp;gt;
Hadoop v1 Commands # 1. Print the Hadoop version # hadoop version 2. List the contents of the root directory in HDFS # # hadoop fs -ls / 3. Report the amount of space used and # available on currently mounted filesystem # # hadoop fs -df hdfs:/ 4. Count the number of directories,files and bytes under # the paths that match the specified file pattern # # hadoop fs -count hdfs:/ 5.</description>
    </item>
    
    <item>
      <title>What is HDFS?</title>
      <link>https://www.wikiod.com/hadoop/what-is-hdfs/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://www.wikiod.com/hadoop/what-is-hdfs/</guid>
      <description>A good explanation of HDFS and how it works.
Syntax should contain the commands which maybe use in HDFS.
Finding files in HDFS # To find a file in the Hadoop Distributed file system:
hdfs dfs -ls -R / | grep [search_term] In the above command,
-ls is for listing files
-R is for recursive(iterate through sub directories)
/ means from the root directory
| to pipe the output of first command to the second</description>
    </item>
    
    <item>
      <title>Introduction to MapReduce</title>
      <link>https://www.wikiod.com/hadoop/introduction-to-mapreduce/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://www.wikiod.com/hadoop/introduction-to-mapreduce/</guid>
      <description>Syntax # To run the example, the command syntax is:
bin/hadoop jar hadoop-*-examples.jar wordcount [-m &amp;lt;#maps&amp;gt;] [-r &amp;lt;#reducers&amp;gt;] &amp;lt;in-dir&amp;gt; &amp;lt;out-dir&amp;gt; To copy data into HDFS(from local):
bin/hadoop dfs -mkdir &amp;lt;hdfs-dir&amp;gt; //not required in hadoop 0.17.2 and later bin/hadoop dfs -copyFromLocal &amp;lt;local-dir&amp;gt; &amp;lt;hdfs-dir&amp;gt; Word Count program using MapReduce in Hadoop.
Word Count Program(in Java &amp;amp; Python) # The word count program is like the &amp;ldquo;Hello World&amp;rdquo; program in MapReduce.
Hadoop MapReduce is a software framework for easily writing applications which process vast amounts of data (multi-terabyte data-sets) in-parallel on large clusters (thousands of nodes) of commodity hardware in a reliable, fault-tolerant manner.</description>
    </item>
    
    <item>
      <title>Hadoop load data</title>
      <link>https://www.wikiod.com/hadoop/hadoop-load-data/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://www.wikiod.com/hadoop/hadoop-load-data/</guid>
      <description>Load data into hadoop hdfs # STEP 1: CREATE A DIRECTORY IN HDFS, UPLOAD A FILE AND LIST CONTENTS
Let’s learn by writing the syntax. You will be able to copy and paste the following example commands into your terminal:
hadoop fs -mkdir: # Takes the path URI’s as an argument and creates a directory or multiple directories.
Usage: # # hadoop fs -mkdir &amp;lt;paths&amp;gt; Example: # hadoop fs -mkdir /user/hadoop hadoop fs -mkdir /user/hadoop/dir1 /user/hadoop/dir2 /user/hadoop/dir3 hadoop fs -put: # Copies single src file or multiple src files from local file system to the Hadoop Distributed File System.</description>
    </item>
    
    <item>
      <title>hue</title>
      <link>https://www.wikiod.com/hadoop/hue/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://www.wikiod.com/hadoop/hue/</guid>
      <description>Hue is an User Interface to connect and work with most of the commonly used Bigdata technologies like HDFS, Hive, Spark, Hbase, Sqoop, Impala, Pig, Oozie etc. Hue also supports running queries against Relational databases.
Hue, a django web application, was primarily built as a workbench for running Hive queries. Later the functionality of Hue increased to support different components of Hadoop Ecosystem. It is available as open source software under Apache License.</description>
    </item>
    
    <item>
      <title>Debugging Hadoop MR Java code in local eclipse dev environment.</title>
      <link>https://www.wikiod.com/hadoop/debugging-hadoop-mr-java-code-in-local-eclipse-dev-environment/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://www.wikiod.com/hadoop/debugging-hadoop-mr-java-code-in-local-eclipse-dev-environment/</guid>
      <description>The basic thing to remember here is that debugging a Hadoop MR job is going to be similar to any remotely debugged application in Eclipse.
A debugger or debugging tool is a computer program that is used to test and debug other programs (the “target” program). It is greatly useful specially for a Hadoop environment wherein there is little room for error and one small error can cause a huge loss.</description>
    </item>
    
  </channel>
</rss>
