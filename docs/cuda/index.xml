<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>cuda Tutorial on </title>
    <link>https://www.wikiod.com/docs/cuda/</link>
    <description>Recent content in cuda Tutorial on </description>
    <generator>Hugo -- gohugo.io</generator><atom:link href="https://www.wikiod.com/docs/cuda/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Getting started with cuda</title>
      <link>https://www.wikiod.com/cuda/getting-started-with-cuda/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://www.wikiod.com/cuda/getting-started-with-cuda/</guid>
      <description>Let&amp;rsquo;s launch a single CUDA thread to say hello # This simple CUDA program demonstrates how to write a function that will execute on the GPU (aka &amp;ldquo;device&amp;rdquo;). The CPU, or &amp;ldquo;host&amp;rdquo;, creates CUDA threads by calling special functions called &amp;ldquo;kernels&amp;rdquo;. CUDA programs are C++ programs with additional syntax.
To see how it works, put the following code in a file named hello.cu:
#include &amp;lt;stdio.h&amp;gt; // __global__ functions, or &amp;quot;kernels&amp;quot;, execute on the device __global__ void hello_kernel(void) { printf(&amp;quot;Hello, world from the device!</description>
    </item>
    
    <item>
      <title>Parallel reduction (e.g. how to sum an array)</title>
      <link>https://www.wikiod.com/cuda/parallel-reduction-eg-how-to-sum-an-array/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://www.wikiod.com/cuda/parallel-reduction-eg-how-to-sum-an-array/</guid>
      <description>Parallel reduction algorithm typically refers to an algorithm which combines an array of elements, producing a single result. Typical problems that fall into this category are:
summing up all elements in an array finding a maximum in an array In general, the parallel reduction can be applied for any binary associative operator, i.e. (A*B)*C = A*(B*C). With such operator *, the parallel reduction algorithm repetedely groups the array arguments in pairs.</description>
    </item>
    
    <item>
      <title>Inter-block communication</title>
      <link>https://www.wikiod.com/cuda/inter-block-communication/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://www.wikiod.com/cuda/inter-block-communication/</guid>
      <description>Blocks in CUDA operate semi-independently. There is no safe way to synchronize them all. However, it does not mean that they cannot interact with each other in any way.
Last-block guard # Consider a grid working on some task, e.g. a parallel reduction. Initially, each block can do its work independently, producing some partial result. At the end however, the partial results need to be combined and merged together. A typical example is a reduction algorithm on a big data.</description>
    </item>
    
    <item>
      <title>Installing cuda</title>
      <link>https://www.wikiod.com/cuda/installing-cuda/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://www.wikiod.com/cuda/installing-cuda/</guid>
      <description>To install CUDA toolkit on Windows, fist you need to install a proper version of Visual Studio. Visual studio 2013 should be installed if you&amp;rsquo;re going to install CUDA 7.0 or 7.5. Visual Studio 2015 is supported for CUDA 8.0 and beyond.
When you&amp;rsquo;ve a proper version of VS on your system, it&amp;rsquo;s time to download and install CUDA toolkit. Follow this link to find a the version of CUDA toolkit you&amp;rsquo;re looking for: CUDA toolkit archive</description>
    </item>
    
  </channel>
</rss>
