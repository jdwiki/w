<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>keras Tutorial on </title>
    <link>https://www.wikiod.com/docs/keras/</link>
    <description>Recent content in keras Tutorial on </description>
    <generator>Hugo -- gohugo.io</generator><atom:link href="https://www.wikiod.com/docs/keras/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Getting started with keras</title>
      <link>https://www.wikiod.com/keras/getting-started-with-keras/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://www.wikiod.com/keras/getting-started-with-keras/</guid>
      <description>Getting Started with Keras : 30 Second # The core data structure of Keras is a model, a way to organize layers. The main type of model is the Sequential model, a linear stack of layers. For more complex architectures, you should use the Keras functional API.
Here&amp;rsquo;s the Sequential model:
from keras.models import Sequential model = Sequential() Stacking layers is as easy as .add():
from keras.layers import Dense, Activation model.</description>
    </item>
    
    <item>
      <title>Dealing with large training datasets using Keras fit_generator, Python generators, and HDF5 file format</title>
      <link>https://www.wikiod.com/keras/dealing-with-large-training-datasets-using-keras-fit_generator-python-generators-and-hdf5-file-format/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://www.wikiod.com/keras/dealing-with-large-training-datasets-using-keras-fit_generator-python-generators-and-hdf5-file-format/</guid>
      <description>Machine learning problems often require dealing with large quantities of training data with limited computing resources, particularly memory. It is not always possible to load an entire training set into memory. Fortunately, this can be dealt with through the use of Keras&amp;rsquo; fit_generator method, Python generators, and HDF5 file format.
This example assumes keras, numpy (as np), and h5py have already been installed and imported. It also assumes that video inputs and labels have already been processed and saved to the specified HDF5 file, in the format mentioned, and a video classification model has already been built to work with the given input.</description>
    </item>
    
    <item>
      <title>Create a simple Sequential Model</title>
      <link>https://www.wikiod.com/keras/create-a-simple-sequential-model/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://www.wikiod.com/keras/create-a-simple-sequential-model/</guid>
      <description>The Sequential model is a linear stack of layers.
Simple Multi Layer Perceptron wtih Sequential Models # You can create a Sequential model by passing a list of layer instances to the constructor:
from keras.models import Sequential from keras.layers import Dense, Activation model = Sequential([ Dense(32, input_dim=784), Activation(&#39;relu&#39;), Dense(10), Activation(&#39;softmax&#39;), ]) You can also simply add layers via the .add() method:
model = Sequential() model.add(Dense(32, input_dim=784)) model.add(Activation(&#39;relu&#39;)) Models must be compiled before use:</description>
    </item>
    
    <item>
      <title>Custom loss function and metrics in Keras</title>
      <link>https://www.wikiod.com/keras/custom-loss-function-and-metrics-in-keras/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://www.wikiod.com/keras/custom-loss-function-and-metrics-in-keras/</guid>
      <description>You can create a custom loss function and metrics in Keras by defining a TensorFlow/Theano symbolic function that returns a scalar for each data-point and takes the following two arguments: tensor of true values, tensor of the corresponding predicted values.
Note that the loss/metric (for display and optimization) is calculated as the mean of the losses/metric across all datapoints in the batch.
Keras loss functions are defined in losses.py
Additional loss functions for Keras can be found in keras-contrib repository.</description>
    </item>
    
    <item>
      <title>Classifying Spatiotemporal Inputs with CNNs, RNNs, and MLPs</title>
      <link>https://www.wikiod.com/keras/classifying-spatiotemporal-inputs-with-cnns-rnns-and-mlps/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://www.wikiod.com/keras/classifying-spatiotemporal-inputs-with-cnns-rnns-and-mlps/</guid>
      <description>Spatiotemporal data, or data with spatial and temporal qualities, are a common occurrence. Examples include videos, as well as sequences of image-like data, such as spectrograms.
Convolutional Neural Networks (CNNs) are particularly suited for finding spatial patterns. Recurrent Neural Networks (RNNs), on the other hand, are particularly suited for finding temporal patterns. These two, in combination with Multilayer Perceptrons, can be effective for classifying spatiotemporal inputs.
In this example, a VGG-16 model pre-trained on the ImageNet database was used.</description>
    </item>
    
    <item>
      <title>Transfer Learning and Fine Tuning using Keras</title>
      <link>https://www.wikiod.com/keras/transfer-learning-and-fine-tuning-using-keras/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://www.wikiod.com/keras/transfer-learning-and-fine-tuning-using-keras/</guid>
      <description>This topic includes short, brief but comprehensive examples of loading pre-trained weights, inserting new layers on top or in the middle of pre-tained ones, and training a new network with partly pre-trained weights. An example for each of out-of-the-box pre-trained networks, available in Keras library (VGG, ResNet, Inception, Xception, MobileNet), is required.
Transfer Learning using Keras and VGG # In this example, three brief and comprehensive sub-examples are presented:
Loading weights from available pre-trained models, included with Keras library Stacking another network for training on top of any layers of VGG Inserting a layer in the middle of other layers Tips and general rule-of-thumbs for Fine-Tuning and transfer learning with VGG Loading pre-trained weights # Pre-trained on ImageNet models, including VGG-16 and VGG-19, are available in Keras.</description>
    </item>
    
  </channel>
</rss>
