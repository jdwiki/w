<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>ibm-watson-cognitive Tutorial on </title>
    <link>https://www.wikiod.com/docs/ibm-watson-cognitive/</link>
    <description>Recent content in ibm-watson-cognitive Tutorial on </description>
    <generator>Hugo -- gohugo.io</generator><atom:link href="https://www.wikiod.com/docs/ibm-watson-cognitive/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Getting started with ibm-watson-cognitive</title>
      <link>https://www.wikiod.com/ibm-watson-cognitive/getting-started-with-ibm-watson-cognitive/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://www.wikiod.com/ibm-watson-cognitive/getting-started-with-ibm-watson-cognitive/</guid>
      <description>Getting API credentials # To authenticate to Watson services, you need credentials for each service that you plan to use. Depending on the service, you will need to pass a username and password with Basic Authentication, or you will need to pass an API key in a parameter for each request you make.
How to get credentials for a Watson service:
Sign up for Bluemix and log in. Go to the service page for your desired Watson service: AlchemyLanguage and AlchemyData News Conversation Dialog Document Conversion Language Translation Natural Language Classifier Personality Insights Retrieve and Rank Speech to Text Text to Speech Tone Analyzer Tradeoff Analytics Visual Recognition Select your desired plan, and click CREATE: Click the &amp;ldquo;Service Credentials&amp;rdquo; button from your service dashboard page to view your credentials.</description>
    </item>
    
    <item>
      <title>Speech to Text</title>
      <link>https://www.wikiod.com/ibm-watson-cognitive/speech-to-text/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://www.wikiod.com/ibm-watson-cognitive/speech-to-text/</guid>
      <description>IBM Watson Speech to Text offers a variety of options for transcribing audio in various languages and formats:
WebSockets – establish a persistent connection over the WebSocket protocol for continuous transcription
Sessionless – transcribe audio without the overhead of establishing and maintaining a session
Sessions – create long multi-turn exchanges with the service or establish multiple parallel conversations with a particular instance of the service
Asynchronous – provides a non-blocking HTTP interface for transcribing audio.</description>
    </item>
    
    <item>
      <title>AlchemyLanguage</title>
      <link>https://www.wikiod.com/ibm-watson-cognitive/alchemylanguage/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://www.wikiod.com/ibm-watson-cognitive/alchemylanguage/</guid>
      <description>AlchemyLanguage is a collection of text analysis methods that provide deeper insight into your text or HTML content. See the Getting Started topic to learn how to get started with AlchemyLanguage and other Watson services. For more AlchemyLanguage details and examples, see the [API reference][2] and [documentation][3].
Size limits # HTML content before text cleaning: 600 KB Source text, after text cleaning: 50 KB Calls that use Custom Models: 5 KB Language support # To see which languages are supported for each function, refer to each function&amp;rsquo;s entry in the [API reference][4].</description>
    </item>
    
    <item>
      <title>Visual Recognition</title>
      <link>https://www.wikiod.com/ibm-watson-cognitive/visual-recognition/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://www.wikiod.com/ibm-watson-cognitive/visual-recognition/</guid>
      <description>Get a list of custom classifiers # This lists all of the custom classifiers you have trained.
&#39;use strict&#39;; let watson = require(&#39;watson-developer-cloud&#39;); var visualRecognition = watson.visual_recognition({ version: &#39;v3&#39;, api_key: process.env[&#39;API_KEY&#39;], version_date:&#39;2016-05-19&#39; }); let url = &amp;quot;https://upload.wikimedia.org/wikipedia/commons/1/1c/Chris_Evans_filming_Captain_America_in_DC_cropped.jpg&amp;quot; visualRecognition.classify({url: url}, function(error, results) { console.log(JSON.stringify(results,null,2)); }); Get information about a specific custom classifier # This returns information about a specific classifier ID you have trained. This includes information about its current status (i.</description>
    </item>
    
    <item>
      <title>Retrieve and Rank</title>
      <link>https://www.wikiod.com/ibm-watson-cognitive/retrieve-and-rank/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://www.wikiod.com/ibm-watson-cognitive/retrieve-and-rank/</guid>
      <description>The Solrj client and the Java SDK are independent so you can update them individually. Always make sure you use the latest version of the Java SDK.
See the GitHub release page for updates https://github.com/watson-developer-cloud/java-sdk/releases
Search and Rank using the Retrieve and Rank in Java # Install the required dependencies:
&#39;org.apache.solr:solr-solrj:5.5.1&#39; &#39;org.apache.httpcomponents:httpclient:4.3.6&#39; &#39;com.ibm.watson.developer_cloud:java-sdk:3.2.0&#39; The code below assumes you have a Solr collection with documents and you have trained a ranker, otherwise follow this tutorial</description>
    </item>
    
  </channel>
</rss>
