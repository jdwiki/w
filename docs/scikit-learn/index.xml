<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>scikit-learn Tutorial on </title>
    <link>https://www.wikiod.com/docs/scikit-learn/</link>
    <description>Recent content in scikit-learn Tutorial on </description>
    <generator>Hugo -- gohugo.io</generator><atom:link href="https://www.wikiod.com/docs/scikit-learn/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Getting started with scikit-learn</title>
      <link>https://www.wikiod.com/scikit-learn/getting-started-with-scikit-learn/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://www.wikiod.com/scikit-learn/getting-started-with-scikit-learn/</guid>
      <description>Installation of scikit-learn # The current stable version of scikit-learn requires:
Python (&amp;gt;= 2.6 or &amp;gt;= 3.3), NumPy (&amp;gt;= 1.6.1), SciPy (&amp;gt;= 0.9). For most installation pip python package manager can install python and all of its dependencies:
pip install scikit-learn However for linux systems it is recommended to use conda package manager to avoid possible build processes
conda install scikit-learn To check that you have scikit-learn, execute in shell:</description>
    </item>
    
    <item>
      <title>Model selection</title>
      <link>https://www.wikiod.com/scikit-learn/model-selection/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://www.wikiod.com/scikit-learn/model-selection/</guid>
      <description>Cross-validation # Learning the parameters of a prediction function and testing it on the same data is a methodological mistake: a model that would just repeat the labels of the samples that it has just seen would have a perfect score but would fail to predict anything useful on yet-unseen data. This situation is called overfitting. To avoid it, it is common practice when performing a (supervised) machine learning experiment to hold out part of the available data as a test set X_test, y_test.</description>
    </item>
    
    <item>
      <title>Classification</title>
      <link>https://www.wikiod.com/scikit-learn/classification/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://www.wikiod.com/scikit-learn/classification/</guid>
      <description>RandomForestClassifier # A random forest is a meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and use averaging to improve the predictive accuracy and control over-fitting.
A simple usage example:
Import:
from sklearn.ensemble import RandomForestClassifier Define train data and target data:
train = [[1,2,3],[2,5,1],[2,1,7]] target = [0,1,0] The values in target represent the label you want to predict.
Initiate a RandomForest object and perform learn (fit):</description>
    </item>
    
    <item>
      <title>Feature selection</title>
      <link>https://www.wikiod.com/scikit-learn/feature-selection/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://www.wikiod.com/scikit-learn/feature-selection/</guid>
      <description>Low-Variance Feature Removal # This is a very basic feature selection technique.
Its underlying idea is that if a feature is constant (i.e. it has 0 variance), then it cannot be used for finding any interesting patterns and can be removed from the dataset.
Consequently, a heuristic approach to feature elimination is to first remove all features whose variance is below some (low) threshold.
Building off the example in the documentation, suppose we start with</description>
    </item>
    
    <item>
      <title>Regression</title>
      <link>https://www.wikiod.com/scikit-learn/regression/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://www.wikiod.com/scikit-learn/regression/</guid>
      <description>Ordinary Least Squares # Ordinary Least Squares is a method for finding the linear combination of features that best fits the observed outcome in the following sense.
If the vector of outcomes to be predicted is y, and the explanatory variables form the matrix X, then OLS will find the vector β solving
minβ|y^ - y|22,
where y^ = X β is the linear prediction.
In sklearn, this is done using sklearn.</description>
    </item>
    
    <item>
      <title>Dimensionality reduction (Feature selection)</title>
      <link>https://www.wikiod.com/scikit-learn/dimensionality-reduction-feature-selection/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://www.wikiod.com/scikit-learn/dimensionality-reduction-feature-selection/</guid>
      <description>Reducing The Dimension With Principal Component Analysis # Principal Component Analysis finds sequences of linear combinations of the features. The first linear combination maximizes the variance of the features (subject to a unit constraint). Each of the following linear combinations maximizes the variance of the features in the subspace orthogonal to that spanned by the previous linear combinations.
A common dimension reduction technique is to use only the k first such linear combinations.</description>
    </item>
    
    <item>
      <title>Receiver Operating Characteristic (ROC)</title>
      <link>https://www.wikiod.com/scikit-learn/receiver-operating-characteristic-roc/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://www.wikiod.com/scikit-learn/receiver-operating-characteristic-roc/</guid>
      <description>Introduction to ROC and AUC # Example of Receiver Operating Characteristic (ROC) metric to evaluate classifier output quality.
ROC curves typically feature true positive rate on the Y axis, and false positive rate on the X axis. This means that the top left corner of the plot is the “ideal” point - a false positive rate of zero, and a true positive rate of one. This is not very realistic, but it does mean that a larger area under the curve (AUC) is usually better.</description>
    </item>
    
  </channel>
</rss>
